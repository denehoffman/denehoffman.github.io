[{"url":"https://denehoffman.com/blog/","title":"String Theory","description":null,"body":"\nA blog that’s maybe about music and maybe about physics (but not about string theory)\n\n","path":null},{"url":"https://denehoffman.com/blog/ganesh-a-new-optimization-crate-for-rust/","title":"ganesh: A New Optimization Crate for Rust","description":null,"body":"For the last year or so, I’ve been working on a new optimization crate for Rust, called ganesh. In this blog post, I’d like to introduce the project, give a quick overview of its features, a neat example of what it can do, and why you might use it instead of popular crates like argmin.\nCurrent State-of-the-Art: argmin\nI want to start by acknowledging argmin and the amazing work that has been done on and with this crate. argmin hosts a ton of really neat optimization algorithms, and it’s clear why its the recommended tool to use when you need to perform complex fitting and minimization problems in Rust. With that said, argmin was not the most approachable crate to a new Rust user. I found that when I used it, I ran into the following issues:\n\nThere seems to be a lot of setup, and it’s rather verbose. For example, here’s some code from the L-BFGS example file:\n\n\nI’ve removed some code here which was just intended to show how to modify settings on the line search algorithm, and some of this code could easily be simplified into fewer lines, but I’m more interested in what the user has to provide. For example, we could write the same code in Python with the scipy library:\n\nWe should obviously not expect Rust to be less verbose than Python, but look specifically at what the user has to provide in the argmin example. You first construct a linesearch, of which there are three options. To someone who is not up to speed with optimization literature, this is the first stumbling block. Which line search do we choose? Let’s assume we know enough to at least look at these examples to see that the line search algorithm of Moré and Thuente is often preferred with the BFGS family of algorithms. Next, we pick a solver. Technically, if you leave out the method='L-BFGS-B argument from the Python code, the BFGS algorithm will be selected, not L-BFGS, since that’s not even an option in scipy. I actually prefer knowing the explicit algorithm being used, so I have no problem with this. However, what is that 7 doing there? The documentation calls it the history size, and indeed it’s what puts the L in L-BFGS (see my post here). What value should go there? It’s not immediately obvious why 7 was chosen, nor what a good value might be for common optimization problems. While I agree the user should be able to set this value if they wish, shouldn’t they also have a sensible default[1]?\nFinally, we come to the part where we actually run the algorithm. The only issue I have here is what happens if you don’t actually assign an initial parameter vector. In such a case, run will return an error, telling you that L-BFGS requires a starting point. While this is true, it bothers me that the user is allowed to run the algorithm without a starting point at all.\nIn ganesh, the same minimization looks something like this:\n\nEverything used to configure the algorithm is in the LBFGSBConfig struct, which can only be initialized by giving a starting position. The other nice feature is the ability to pass in non-parameter arguments to the problem. This will come in handy in the example later.\n\n\nNo default gradient is provided. It could be argued that this is more a matter of simplicity than a real issue with argmin, but analytic gradients are often nontrivial or at the very least annoying to calculate and write down. The argmin repository offers the finitediff crate, which can be used fairly easily, but in my opinion, the user shouldn’t have to worry about providing a gradient at all unless they want to. ganesh comes with default central finite difference gradients and Hessians which can be easily overwritten if desired.\n\n\nNone of the (native) algorithms handle situations where parameters have bounds. If you want box bounds in argmin, you’ll have to use an external algorithm like egobox or cobyla, the first of which is probably the most straightforward for box bounds. However, let’s consider again how scipy.minimize handles this. Bounds are passed to scipy.minimize and they work on the Nelder-Mead, L-BFGS-B, TNC, SLSQP, Powell, trust-constr, COBYLA, and COBYQA methods. The first thing you might notice there is that the Nelder-Mead algorithm in scipy supports bounds! This is not the case for the argmin implementation. The next algorithm, L-BFGS-B, explicitly supports bounds, as the second “B” implies. This algorithm is similar to L-BFGS and indeed reduces to L-BFGS when no bounds are given, so it tends to be used in place of L-BFGS. This is why ganesh doesn’t even provide a BFGS or L-BFGS implementation (although I did write implementations in earlier versions). To add parameter bounds to the previous example, the user might write something like this:\n\n\n\nThere are other algorithms which support bounds out of the box, but what if we want to use an algorithm which doesn’t? This problem has also been solved (sort of) by projects like LMFIT and MINUIT. To understand how they handle bounds, imagine the problem of fitting a Gaussian probability distribution to some data (imagine we set up a maximum likelihood problem). We might have two free parameters, a mean $\\mu$ and a standard deviation $\\sigma$:\n$$ f(\\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{1}{2}\\frac{(x-\\mu)^2}{\\sigma^2}\\right). $$\nNow if we weren’t looking at this equation and just thought about the parameters, it wouldn’t make a lot of sense to have $\\sigma &lt; 0$. However, when we examine the problem, we find that this doesn’t actually lead to invalid solutions, since we always square $\\sigma$ anyway: we could perform our fit and just use the absolute value of $\\sigma$. Similarly, if we ever had a problem where a parameter $x$ came with the constraint $x &gt;= 0$, we could always rewrite our problem with $x \\to \\sqrt{x^2}$. This does have the downside of giving two ambiguous solutions to the problem, and more if there are multiple parameters defined this way, but we can get around this by defining a set of internal parameters $x_\\text{int}$ which span an unbounded space and a set of transformations to external parameters $x_\\text{ext}$ which preserve the given boundaries. There are infinite ways to define such transformations, but the ones which LMFIT and MINUIT use are as follows:\nUpper and lower bounds:\n$$ x_\\text{int} = \\arcsin\\left(2\\frac{x_\\text{ext} - x_\\text{min}}{x_\\text{max} - x_\\text{min}} - 1\\right) $$\n$$ x_\\text{ext} = x_\\text{min} + \\left(\\sin(x_\\text{int}) + 1\\right)\\frac{x_\\text{max} - x_\\text{min}}{2} $$\nUpper bound only:\n$$ x_\\text{int} = \\sqrt{(x_\\text{max} - x_\\text{ext} + 1)^2 - 1} $$\n$$ x_\\text{ext} = x_\\text{max} + 1 - \\sqrt{x_\\text{int}^2 + 1} $$\nLower bound only:\n$$ x_\\text{int} = \\sqrt{(x_\\text{ext} - x_\\text{min} + 1)^2 - 1} $$\n$$ x_\\text{ext} = x_\\text{min} - 1 + \\sqrt{x_\\text{int}^2 + 1} $$\nHowever, I’ve found these are actually quite difficult to work with at times. If you actually take the positive branch of the square roots, you’ll find that the transformations are not actually inverses of each other! For this reason, I have switched to using a different set of transformations:\nUpper and lower bounds:\n$$ x_\\text{int} = \\frac{u}{\\sqrt{1 - u^2}} $$\nwhere\n$$ u = \\frac{x_\\text{ext} - c}{w},\\ c = \\frac{x_\\text{min} + x_\\text{max}}{2},\\ w = \\frac{x_\\text{max} - x_\\text{min}}{2} $$\n$$ x_\\text{ext} = c + w \\frac{x_\\text{int}}{\\sqrt{x_\\text{int}^2 + 1}} $$\nUpper bound only:\n$$ x_\\text{int} = \\frac{1}{2}\\left(\\frac{1}{(x_\\text{max} - x_\\text{ext})} - (x_\\text{max} - x_\\text{ext}) \\right) $$\n$$ x_\\text{ext} = x_\\text{max} - (\\sqrt{x_\\text{int}^2 + 1} - x_\\text{int}) $$\nLower bound only:\n$$ x_\\text{int} = \\frac{1}{2}\\left((x_\\text{ext} - x_\\text{min}) - \\frac{1}{(x_\\text{ext} - x_\\text{min})} \\right) $$\n$$ x_\\text{ext} = x_\\text{min} + (\\sqrt{x_\\text{int}^2 + 1} + x_\\text{int}) $$\nNow any algorithm which does not support bounds can simply operate on the internal parameters and the user can transform these into external parameters on which the function can be evaluated! ganesh provides methods for adding such transformations to an algorithm, at least in cases where it makes sense. Unfortunately, this method does not come without drawbacks. The transformation functions themselves add to the evaluation time, especially when it comes to computing gradients or Hessians, but even worse, they transform linear problems into nonlinear ones! This also leads to issues with correctly calculating covariance matrices, and the MINUIT documentation recommends running a fit without bounds after a bounded fit converges[2].\nIt should be noted that L-BFGS-B has been a desired algorithm by at least a couple of users of argmin for several years without any visible progress (it appears that the current maintainer is a bit busy, I know the feeling). If I have enough time, I might volunteer to port my own code to argmin, and if any of the current contributors are reading this, they have my permission to do just that with attribution.\n\n\t\n\t\tNote\n\tAll of the issues I have with argmin are not reasons not to use it, and this blog post isn’t intended as a hit piece. I’m just highlighting the reasons I decided to write my own crate and why I think it is a good addition to the existing Rust optimization ecosystem! I believe that argmin excels in its customizability, and ganesh should be seen as an alternative approach which I believe is a bit more user-friendly with sensible defaults.\n\n\nA New Optimization Crate: ganesh\nI started working on ganesh as part of my PhD in particle physics. For the uninitiated, the data collected from particle accelerators are distributed across many kinematic variables, such as the energy and momentum of particles measured by various detectors. These data follow mathematical patterns, functions which we call “amplitudes”. To determine the angular momentum (or spin) of some particle, we can show that its decay products follow spherical harmonic distributions, and we perform an “amplitude analysis” by fitting decay distributions to sets of spherical harmonics (this specific technique is often referred to as a “partial-wave analysis”). It is in this stage that ganesh was born, as I was simultaneously developing a Rust library which performs such analyses. At first I was content to provide functions which users could fit with their own optimization libraries, e.g. writing Python bindings and using scipy or iminuit to perform the fits, but I decided it would also be nice for users to just ask the same library to do the fit for them. This meant either wrapping existing optimization libraries in Python, or adding in an optimization crate in Rust. After experimenting with the first option, I decided to try the second (since I wanted this crate to be usable without Python), starting with argmin. At first, this seemed to be a good direction, but the lack of parameter bounds in any of the native methods made it difficult to set up some minimizations. For example, the probability density of a relativistic Breit-Wigner (commonly used to model the mass of a decaying particle) contains a width parameter that sometimes does not appear in a square, like the Gaussian distribution I showed above. Of course, I could just do what MINUIT does and wrap every bounded parameter in a nonlinear transformation, and indeed this is the approach I took with algorithms in ganesh which don’t natively support bounds, but for BFGS-like solvers, this was just repeating the mistakes of MINUIT by making users wary of the covariance of fit results. Another major reason for developing my own crate was to implement algorithms which argmin isn’t really suited for, like Markov-chain Monte Carlo (MCMC). This isn’t really an optimization technique, but rather a technique for exploring a likelihood landscape. There are many ways to define the convergence of such an algorithm, but argmin only allows downstream users to define Observers, which cannot terminate an algorithm (as far as I know).\nThe design philosophy is that of sensible defaults rather than overwhelming choice, and the goal is to make it as simple as possible to get started with ganesh without having to spend too much time learning the inner workings. I’ll admit that some of the above complaints could have been solved with some lengthy PRs to argmin, but a lot of the design changes I wanted would involve large breaking changes, and I don’t see the harm in offering an alternative crate for those who are interested in using it (or hopefully developing it further).\nFeatures\nOne of the biggest features of ganesh is the L-BFGS-B algorithm itself, as it is (as far as I know) the only pure-Rust implementation. While there are other crates which offer the algorithm, they all either bind to the original FORTRAN library or to a C/C++ library which either binds to that FORTRAN code or ports it directly. I really wanted a solution free of dependencies on outside languages, so I used it as a learning opportunity, starting from BFGS and moving towards the limited-memory and bounded versions. I have since removed BFGS and L-BFGS, since they tend to be just as fast as L-BFGS-B anyway, so there doesn’t seem to be much to gain from keeping them around.\nAs I alluded to earlier, custom termination conditions are another feature of ganesh. In particular, any Terminator-implementing struct which satisfies the right trait bounds can be used with any algorithm. While reading some of the original Nelder-Mead papers, I found that there are several termination conditions, some of which work better than others depending on the topology of the problem. I provided implementations of several different conditions, as well as reasonable defaults according to more modern papers, but the crate is designed to allow for future support of custom conditions without modifying core code.\nI also mentioned MCMC algorithms. In Python, I was very used to using both the Ensemble Slice Sampler (ESS) of zeus and the Affine Invariant Markov chain Monte Carlo Ensemble Sampler (AIES) from emcee. Both of these are fantastic libraries, and I decided to implement both as ganesh’s first two MCMC algorithms. AIES is not yet entirely up-to-date with emcee, and I have not gotten around to adding parallelization (though I plan to in the future[3]), but both are currently usable.\nThe project recently gained its first contributor, estriv, who has been behind much of the recent API update. He additionally added the current implementation of the simulated annealing algorithm, and has helped me outline how we want the crate to operate and the interface we want to provide to the end-user.\nExample\nTo show off ganesh, I’ll implement a simple but fun example. Let’s imagine we have some dataset which we expect follows a multivariate normal distribution, and we want to fit a model to it. We’ll start by generating a toy dataset:\n\nganesh comes with a nice trait which extends fastrand’s Rng with some useful methods, like mv_normal, which as the name suggests, generates data according to a multivariate normal distribution. Let’s also write some code to save this to a Python .pkl file. I’m not very up to date on Rust-based plotting libraries, so I’m just going to use matplotlib for now:\n\nLet’s run it:\n\nLet’s plot the result, nothing fancy:\n\n\nNow we should get on to fitting these data to a model. For this, we’ll first define the probability distribution function for a multivariate Gaussian:\n$$ \\Pr(\\vec{x}; \\vec{\\mu}, \\mathbf{\\Sigma}) = \\exp\\left[-\\frac{1}{2}(\\vec{x} - \\vec{\\mu})^{\\intercal} \\mathbf{\\Sigma}^{-1} (\\vec{x} - \\vec{\\mu})\\right] (2\\pi)^{-k/2}|\\mathbf{\\Sigma}|^{-1/2}, $$\nwhere $k$ is the dimension of the distribution. To actually perform the fit, we might instead want to maximize the likelihood function,\n$$ \\mathcal{L}(\\vec{X}; \\vec{\\mu}, \\mathbf{\\Sigma}) = \\prod_{i=0}^{N} \\Pr(\\vec{X}_i; \\vec{\\mu, \\mathbf{\\Sigma}}), $$\nwhere $\\vec{X}$ is our dataset of size $N$. For those familiar with this kind of problem, you’ll know what to do next, but for those who aren’t, note that the probability distribution function should give values between $0$ and $1$, so a product of a bunch of those numbers will tend towards a very small value, especially when you get more data. This is not very nice to deal with computationally, so we’ll instead take the natural logarithm of this likelihood function. Since the natural log is strictly increasing, this mapping will not change the value of the parameters at the function’s maximum, it will just rescale the problem from a range of $[0, 1]$ to a range of $(-\\infty,0]$. Next, since most of our algorithms are intended for minimization rather than maximization, we will instead use $-2\\ln(\\mathcal{L})$, where the factor of $2$ is mostly convention and has to do with proper covariance scaling. This simplifies our function greatly:\n$$ -2\\ln(\\mathcal{L(\\vec{X}; \\vec{\\mu}, \\mathbf{\\Sigma})}) = -\\sum_{i=0}^{N} \\left[(\\vec{X}_i - \\vec{\\mu})^{\\intercal} \\mathbf{\\Sigma}^{-1} (\\vec{X}_i - \\vec{\\mu})\\right] - N\\left[k\\ln(2\\pi) + \\ln|\\mathbf{\\Sigma}|\\right]. $$\nNext, note that any symmetric positive-definite matrix may be decomposed by Cholesky factorization into $\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}^{\\intercal}$ where $\\mathbf{L}$ is lower triangular with $L_{ii}&gt;0$. This means that the determinant in that last term can be calculated as $|\\mathbf{\\Sigma}| = |\\mathbf{L}||\\mathbf{L}^\\intercal| = |\\mathbf{L}|^2 = \\prod_{j=0}^k L_{jj}$, so $\\ln|\\mathbf{\\Sigma}| = 2\\sum_{j=0}^k \\ln L_{jj} $. We will also find it convenient to calculate the term inside the sum via Cholesky factorization, but the general form we want to reproduce is now:\n$$ -2\\ln(\\mathcal{L(\\vec{X}; \\vec{\\mu}, \\mathbf{\\Sigma})}) = -\\sum_{i=0}^{N} \\left[(\\vec{X}_i - \\vec{\\mu})^{\\intercal} \\mathbf{\\Sigma}^{-1} (\\vec{X}_i - \\vec{\\mu})\\right] - N\\left[k\\ln(2\\pi) + 2\\sum_{j=0}^k \\ln L_{jj} \\right]. $$\nIn code, this looks like,\n\nLet’s perform the fit now, using the Nelder-Mead algorithm:\n\nNow this will work, but mostly by coincidence. We haven’t actually done anything to ensure the algorithm sticks to covariance matrices which are positive-semidefinite. Just to be clear, there are a few problems if that happens to not be the case. First, The determinant will be negative and the square root of a negative number will be imaginary, making our probability distribution function imaginary. Second, the term in the exponent may end up being positive, which willl make it impossible to normalize the function over all reals. It just so happens that the starting point we chose constructs a simplex where every point gives a positive-semidefinite matrix and the algorithm happens to never traverse a position where this is not the case for any simplex point. We can easily break that by choosing a different starting point:\n\nThe default simplex construction method for a five-dimensional problem takes the given starting point and constructs five other points by scaling each coordinate by $1.05$, one at a time. This means that one of those starting points is\n$$ \\begin{bmatrix} 1.0 &amp; 1.05 \\\\ 1.05 &amp; 1.0 \\end{bmatrix}, $$\nwhich is not positive-semidefinite! And of course, even if we pick a nice starting point, there is no guarantee that we never hit such a point somewhere down the line, causing sigma.cholesky().unwrap() to panic. While we might just try to be careful about the first failure point in picking a good starting simplex, there is a neat way to get around the second issue: a change of variables. As it turns out, if instead operate on values of the lower-triangular matrix $\\mathbf{L}$ described in the Cholesky decomposition, the only requirement we have is that the diagonal entries of the matrix are positive, which can be handled with some sort of constraint or a further change of variables. ganesh comes with an interface for creating these change-of-variables transformations, and we can simply construct a transformation that allows our algorithm to operate in more convenient space, such as the the space of lower-triangular matrices $\\mathbf{L}$ such that $\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}^{\\intercal}$. Any such lower-triangular matrix will work, provided the diagonal entries are positive (and nonzero), a property we can also ensure with a built-in transformation:\n\n\nNote that this technically doesn’t allow for matrices which are positive-semidefinite but not positive-definite, but such matrices lead to degenerate Gaussians which we will assume are not of interest for our purposes. This also won’t get us past the posibility that the construction of the simplex might fail, but at least it will panic with a nice message in that case.\nIf we run this code, we get the result,\n$$ \\vec{\\mu} = \\begin{bmatrix} 1.1958 \\\\ 2.3009 \\end{bmatrix}, \\quad \\mathbf{\\Sigma} = \\begin{bmatrix} 0.5955 &amp; 0.4896 \\\\ 0.4896 &amp; 0.6814 \\end{bmatrix}, $$\nwhich is very close to our truth values of\n$$ \\vec{\\mu} = \\begin{bmatrix} 1.2000 \\\\ 2.3000 \\end{bmatrix}, \\quad \\mathbf{\\Sigma} = \\begin{bmatrix} 0.6000 &amp; 0.5000 \\\\ 0.5000 &amp; 0.7000 \\end{bmatrix}. $$\nThe question now becomes, how close? We didn’t exactly calculate the uncertainties of the fit. Since Nelder-Mead is a gradient-free method, we didn’t even define a way to calculate the Hessian. We could calculate the Hessian manually, but instead let’s see how transformed algorithms can do this automatically. All we have to do is swap out our Nelder-Mead algorithm with a gradient-based algorithm like L-BFGS-B:\n\nThis time I’ll just show you what that final print statement gives, since it’s a bit more informative now:\n\nHere we have some information about our fit, including the negative log-likelihood at the minimum, the number of times the function and its gradient are called, a message about how the algorithm converged, and a list of the parameters with their values at termination, the uncertainties on those values, the initial values, and information about bounds on those parameters (none here, this doesn’t include bounds we placed using internal tranformations, just bounds on the external parameters via the algorithm itself). As you can see, this is nearly the same minimum as the one obtained via Nelder-Mead up to the first three decimal places.\nThere is another way we can get parameter errors, and that’s via MCMC. As mentioned, ganesh currently supports two major MCMC algorithms, AIES and ESS. These are technically very different algorithms, but in practice they both work well for most “normal” problems. I tend to like ESS because it has a guaranteed 100% step acceptance rate by construction, although more work goes into proposing a step in a chain. Both methods are ensemble samplers, so unlike a basic algorithm like Metropolis-Hastings, the next position of each walker depends on the positions of all walkers in the ensemble. This makes both of them better at problems where parameters exist on very different overall scales. I won’t go through the details here, but you can read about them here[4] and here[5]. For this problem, we’ll use the ESS algorithm. To determine when to stop the sampler, we could set a maximum number of steps, but for this example, we’ll use a different metric, the autocorrelation time, $\\tau$. Autocorrelation times measure how correlated a walker’s chain of positions is with its past positions. Since we will chose an arbitrary starting distribution for our walkers, it is important that our final distributions do not depend on this initial distribution, so the autocorrelation time tells us how many steps we need to get rid of to satisfy this condition. Given that, we can use this as a stopping criterion. We will simply calculate $\\tau$ every few steps and see if we have taken more than $M \\tau$ steps total, where $M$ is at least $1$. We can then decide to burn (get rid of) the first $M_b \\tau$ steps where $M_b &lt; M$. Additionally, since we are not checking $\\tau$ every step (we could, but it’s not efficient), we also require the change in $\\tau$ to be less than some threshold, ensuring that enough steps have been taken to effectively measure $\\tau$. We also discard some fraction of steps at the beginning of the chain in this calculation, since we not only care about correlations with the initial position, we also care about correlations with any position the walker encountered sufficiently long ago. All of this logic is built into a ganesh Terminator, which provides useful defaults but is also fully customizable. The MCMC code looks something like this:\n\nTo send this data to Python for plotting, I wrote some additional functions to store the fit result and nested vectors:\n\nThen in Python, we’ll use the corner library to make some nice plots:\n\n\n\n\n\n\nThis last plot is probably the most interesting to look at, since it shows us the actual posterior distribution of our parameters. In blue, we have the truth values used to generate the data, and in red we have the values obtained in our fit. While we are within $3\\sigma$ of each truth value, the fact that the fit is far away from the means of the MCMC samples is not great, but it just means the algorithm probably terminated too soon and gave us a less-accurate fit. We can also see here that the MCMC parameter errors are very close to the symmetric errors we obtained from L-BFGS-B. The difference is that the ESS algorithm didn’t actually need a gradient to compute these, and due to some fun math, the transforms we applied were taken into account when L-BFGS-B directly calculated the Hessian.\nFor my final act, I’ll show off a neat library I found called matplotloom which simplifies the process of making cool animations with matplotlib. I won’t go into detail about how this code works, but it should be fairly straightforward:\n\n\nThis entire example can be found (in a slightly more condensed form) in the examples/ directory of ganesh. See the .justfile in that directory for more information about how to run this and other examples.\nI hope this has been an informative look into ganesh. The current goal for a v1.0.0 release is to add a more diverse set of algorithms. Also, there is probably some low-hanging fruit in terms of algorithm optimizations. If you’re interested in adding an algorithm, optimizing an existing algorithm, or adding a new feature, ganesh is open to contrigutions on GitHub. I will be posting some issues representing some of the more desired algorithms and features soon.\n\n\nNearly every other implementation I’ve come across uses 10, although Liu and Nocedal[6] recommend $3\\leq m \\leq 7$. ↩\n\n\nThis is a bit silly, since MINUIT is essentially just the BFGS algorithm with this bounds transformation, so this could all be avoided by just using L-BFGS-B. The technical reason for this is that MINUIT doesn’t bother to calculate higher-order derivatives of the transformation functions and uses a linear approximation instead. ganesh does include these higher-order derivatives for bounds transformations. ↩\n\n\nI’ve been avoiding this partially because the functions I use in my research are already parallelized over their underlying datasets, and nested parallel loops are a bit of a pain to get right. ↩\n\n\nJ. Goodman and J. Weare, “Ensemble samplers with affine invariance,” CAMCoS, vol. 5, no. 1, pp. 65–80, Jan. 2010, doi: 10.2140/camcos.2010.5.65. ↩\n\n\nM. Karamanis and F. Beutler, “Ensemble slice sampling,” Stat Comput, vol. 31, no. 5, Aug. 2021, doi: 10.1007/s11222-021-10038-2. ↩\n\n\nD. C. Liu and J. Nocedal, “On the limited memory BFGS method for large scale optimization,” Mathematical Programming, vol. 45, no. 1–3, pp. 503–528, Aug. 1989, doi: 10.1007/bf01589116. ↩\n\n\n","path":null},{"url":"https://denehoffman.com/blog/the-bfgs-algorithm-family-in-rust-part-3/","title":"The BFGS Algorithm Family in Rust (Part 3)","description":null,"body":"In my previous posts (here and here), I discussed line searches and the original BFGS algorithm. The next improvement on this algorithm is a limited memory version, called L-BFGS, but rather than explicitly constructing it, we will instead skip to the limited memory version with parameter bounds, L-BFGS-B. However, we should start by understanding why there even is an L-BFGS algorithm at all.\nIn the BFGS algorithm, we start with a guess for the inverse Hessian $B_0^{-1}$ and then improve this guess at each step by defining\n$$ s_k = x_{k+1} - x_k,$$\n$$ y_k = g_{k+1} - g_k,$$\nand\n$$ \\rho = \\frac{1}{\\vec{y}_k^{\\intercal} \\vec{s}_k}, $$\nand updating according to the formula\n$$ B_{k+1}^{-1} = \\left(I - \\rho_k \\vec{s}_k \\vec{y}_k^{\\intercal} \\right) B_k^{-1} \\left(I - \\rho_k \\vec{y}_k \\vec{s}_k^{\\intercal} \\right) + \\rho_k \\vec{s}_k \\vec{s}_k^{\\intercal}. $$\nThis doesn’t seem like it uses too much memory. At worst, the Hessian must be symmetric, so for $n$ parameters, we need $n(n+1)/2$ stored values, and if each is a 64-bit float, that’s just over $40$ kilobytes for $100$ free parameters. However, we must also consider the storage limitations of the 1990s, where typical computers had maybe $128$ megabytes of RAM at a maximum. Some part of that RAM had to be used to store any data used in the minimization, part of it had to store the program itself, and part of it was allocated to the other operations of the computer. We live in an era where most laptops have a minimum of $4$ gigabytes of RAM, and it would take a $16000$-parameter model just to use $1$ gigabyte of that. There are other considerations, for instance, you actually need to allocate twice as much memory since you need some place to store the next iteration. It turns out, there is a way to greatly reduce the amount of memory needed, and this is what Byrd, Nocedal, and Schnabel[1] did back in 1989.\nThe L-BFGS Algorithm\nTheir method is centered around the following idea: if you were to store every $\\vec{s}$ and $\\vec{y}$ and recompute the inverse Hessian from scratch at each step, only the most recent updates to those vectors would be very useful. If we’re 100 steps into the algorithm, we could imagine starting the algorithm at step 90 with the identity matrix initial guess, and eventually we will converge upon the correct Hessian again. In fact, they found that this convergence tends to happen very quickly, typically in less than 10 steps or so. Therefore, instead of storing the entire inverse Hessian, we can instead store some finite number of $\\vec{s}$ and $\\vec{y}$ entries, discarding the oldest ones, and calculate the inverse Hessian from scratch each time. If we store $m$ entries, recall that each vector contains $n$ values, so we now need to only store $2n\\times m$ values rather than $n(n+1)/2$. If we set $m=10$ and again use 64-bit floats, a $100$-parameter model would only need $16$ kilobytes rather than $40$. The real gains are obvious in the case of extremely large models. For instance, the $16000$-parameter model would now only take $2.6$ megabytes rather than $1$ gigabyte. What do we lose here? Well, we typically have to do a few more operations, but we get improvements in cache locality, since we no longer have to transfer a large matrix to the local cache at each step. We also save on allocations, since we don’t actually need to construct the inverse Hessian at all, and we can just construct the update step directly instead.\nAs mentioned, we could go through the entire algorithm separately, but it’s so closely related to L-BFGS-B that we will just implement that directly instead. This article will follow the work of Byrd, Lu, Nocedal, and Zhu[2], particularly their notation, except for my usage of $B^{-1}$ as the inverse Hessian approximate rather than their use of $H$ for that matrix.\nFirst, we define our registers of $\\vec{s}$ and $\\vec{y}$ as $n\\times m$ matrices:\n$$ Y_k = [\\vec{y}_{k-m},\\ldots,\\vec{y}_{k-1}] $$\nand\n$$ S_k = [\\vec{s}_{k-m},\\ldots,\\vec{s}_{k-1}] $$\nJust to clarify, these are matrices where each row corresponds to a particular parameter and each column corresponds to an algorithm step, where the leftmost column is the oldest step and the rightmost column is the most recent. If we let $\\theta$ be a positive scaling parameter, then as long as each pair of $\\vec{s}$ and $\\vec{y}$ satisfy $\\vec{s}^{\\intercal} \\vec{y} &gt; 0 $, we can write the Hessian approximate as\n$$ B_k = \\theta I - W_k M_k W_k^{\\intercal} $$\nwhere\n$$ W_k = \\begin{bmatrix} Y_k &amp; \\theta S_k \\end{bmatrix} $$\nis a block matrix of dimension $n\\times 2m$ and\n$$ M_k = \\begin{bmatrix} -D_k &amp; L_k^{\\intercal} \\\\ L_k &amp; \\theta S_k^{\\intercal} S_k \\end{bmatrix}^{-1} $$\nis another block matrix of dimension $2m\\times 2m$, where\n$$ (L_k)_{i,j} = \\begin{cases} (s_{k-m-1+i})^{\\intercal} (y_{k-m-1+j}) &amp; \\text{if } i &gt; j \\\\ 0 &amp; \\text{otherwise}\\end{cases} $$\nand\n$$ D_k = \\text{diag}\\left[ s_{k-m}^{\\intercal} y_{k-m},\\ldots,s_{k-1}^{\\intercal} y_{k-1} \\right] $$\nWoah, slow down! What’s going on here? This is the notation used in Byrd et al., and it’s pretty confusing. Let’s ignore the intricacies of $M_k$ and just focus on the blocks. $L_k$ is upper triangular with zeros on the diagonal, that’s all the case-statement means. All of those subscripts look confusing, but if you recall, $k-m$ is just shorthand for the oldest index stored and $k-1$ is the newest index, so $L_k$ is really just the upper triangular block of $S_k^{\\intercal} Y_k$! Similarly, $D_k$ is just the diagonal of that matrix, so to get all of these parts, we just need to perform the matrix product and pull apart the relevant blocks. The fact that $M_k$ is the inverse of this matrix seems a bit tricky, but recall that $M_k$ has dimension $2m \\times 2m$, and if we pick $m$ to be small, the cost to compute the inverse is actually pretty low (we can actually make $m$ arbitrarily small (but $ &gt;1 $), and all we will lose is the speed of convergence).\nI’m not going to explain exactly why this set of matrices works, but hopefully you can see that we end up doing something similar to the original BFGS update step, where the Hessian is approximated by some correction to the identity.\nWhile I won’t show the entire Rust code for this, I’ll give you the general idea:\n\nIn the implementation, we also store w_mat and m_mat, since m_mat is just $2m \\times 2m$ and w_mat is $n \\times 2m$, so they’re both relatively small for small $m$. Another thing to note is that the authors suggest discarding correction pairs $\\vec{s}_k$ and $\\vec{y}_k$ if they do not satisfy the curvature condition\n$$ \\vec{s}_k^{\\intercal} \\vec{y}_k &gt; \\varepsilon ||\\vec{y}_k||^2 $$\nfor some small $\\varepsilon &gt; 0$. We technically have a way to compute the inverse Hessian approximate, and we could just stop there and redo the BFGS update step, giving us the L-BFGS algorithm. Let’s instead move on to the bounded version, and we’ll show how we can avoid constructing the inverse Hessian approximate entirely by just directly updating the descent step.\nAdding Boundaries\nThere are a couple of ways one might add bounded parameter support to a minimization algorithm. Algorithms like MINUIT and LMFIT use a nonlinear transform which stretches the distance to a boundary infinitely as one approaches the bound. One advantage of this is that it can generally be adapted to any minimization method, but the major disadvantage is that it turns any linear problem into a nonlinear one. This also makes it difficult to properly reconstruct the Hessian later for uncertainty analysis, and it can be very slow when the minimum is close to or on a boundary.\nAnother way is to pin parameters at their boundary value when an update step would try to cross that boundary. This is kind of what we’ll end up doing, but it’s not quite so straightforward when we want to be memory/computationally efficient.\nComputing the Generalized Cauchy Point\nThe first step is similar to any gradient-descent algorithm. We need to compute a step direction, and originally we’d just go in the opposite direction of the gradient. However, the gradient might point us towards a boundary, or we might already be on a boundary. Remember, we are modeling the objective function locally as a quadratic and jumping as close to the minimum as we can. Byrd et al. start this process by computing “breakpoints”:\n$$ t_i = \\begin{cases}\n(x_i - u_i) / g_i &amp; \\text{if } g_i &lt; 0\\\\\n(x_i - l_i) / g_i &amp; \\text{if } g_i &gt; 0\\\\\n\\infty &amp; \\text{if } g_i = 0\n\\end{cases} $$\nI would forgive the reader if they can’t figure out why we are doing this, I also had no idea what was going on here when I first read it. We calculate the gradient $\\vec{g}$, and we want to move in the opposite direction. For each component of $\\vec{g}$, $g_i &lt; 0$ would imply that we are moving in the positive direction, so we might have to worry about an upper bound. $(x_i - u_i)$ is the distance to that upper bound (but note that the order makes it a negative value as long as we haven’t crossed this bound!), and we divide this by the length of the step (which is also negative according to the condition, so this quantity is strictly positive). Similarly, if the step is in the other direction, we get another strictly positive value with the same kind of proportionality. Assuming no step ever puts us over a boundary, can get a minimum value of zero if we are already on the boundary, otherwise we will get larger positive values the further away from the boundary we are (scaled by the size of the step in that direction).\nNext, we’re going to sort the indices by these values, but first let’s additionally calculate a vector $\\vec{d}$ which follows the formula,\n$$ d_i = \\begin{cases}\n0 &amp; \\text{if } t_i = 0\\\\\n-g_i &amp; \\text{otherwise}\n\\end{cases} $$\nRecall that $t_i = 0$ means $x_i$ is already at a boundary, so this $\\vec{d}$ is just the descent vector (negative gradient) in all directions that aren’t at a boundary. In rust, this looks something like,\n\nWe use $t_i &lt; \\epsilon$ here rather than $t_i = 0$ to avoid issues with comparing floating point values. The point is, if we are close enough to a boundary, we probably shouldn’t try to move in that direction, even if we are technically not right on it. d is mutable for reasons which will become clear later.\nWe next need to establish a list of “free” indices $\\mathcal{F} = \\left\\{i \\mid t_i &gt; 0\\right\\}$. These are the indices of all parameter which are not at a boundary. This next part is a bit tricky, and I’m not going to derive it because this article would be much too long (and you can see the derivation in [2] anyway), but I’ll give you the general idea. We are approximating the function as a quadratic\n$$ m_k(x) = f(x_k) + g_k^{\\intercal}(x - x_k) + \\frac{1}{2}(x - x_k)^{\\intercal}B_k(x - x_k) $$\nbut instead of minimizing it for all $x$, we instead want to minimize it just for feasible values of $x$ (values within the boundaries). We therefore construct a piecewise function $\\vec{x}(t) = P(\\vec{x}_k - t \\vec{g}_k, \\vec{l}, \\vec{u})$ where $P(\\vec{a}, \\vec{l}, \\vec{u})$ just clips $\\vec{a}$ inside a box defined by $\\vec{l}$ and $\\vec{u}$. We then want to find the first minimizer of $q_k(t) = m_k(\\vec{x}(t))$, which we call the generalized Cauchy point $\\vec{x}^c$. This can be thought of as the first feasible point along the search direction (accounting for boundaries). After this, we still need to minimize the objective function along this direction. Imagine that we are walking along the gradient and we periodically hit a boundary in some dimension. Each time we hit a boundary, we pin that parameter to the boundary and keep walking in the same direction for all other parameters. This creates a piecewise-linear path for which the generalized Cauchy point is the first valid point we compute. Now it might make more sense why we defined each $t_i$ as above. We will eventually sort these $t_i$ to determine the order in which to drop free indices, and travel along this piecewise path to find the minimum! Each $t_i$ represents the distance along the piecewise path in units of the gradient which must be traveled to hit a boundary.\nWe initialize some vector $\\vec{p} = W^{\\intercal} \\vec{d}$ which is a vector of dimension $2m$ which holds $Y_k \\vec{d}$ in the first $m$ indices and $\\theta S_k \\vec{d}$ in the next $m$ indices. We also keep track of a vector $c$ initialized to zero with the same dimensions as $\\vec{p}$. We can calculate the derivative of the objective function at the current point to be $f’ = -\\vec{d}^{\\intercal}\\vec{d}$ and the second derivative as $f’’ = -\\theta f’ - \\vec{p}^{\\intercal} M \\vec{p}$ (this comes from a simplification of $f’’ = \\theta \\vec{d}^{\\intercal}\\vec{d} - \\vec{d}^{\\intercal} WMW^{\\intercal} \\vec{d}$). We can then find the distance to the minimum along this direction, $\\Delta t_{\\text{min}} = - \\frac{f’}{f’’}$.\nThis gives us the distance along the first piecewise-linear segment which minimizes the quadratic approximation, so now we need to check the subsequent segments. We do this by sorting $\\mathcal{F}$ by the values of $t_i$ (the smallest $t_i$ corresponds to the parameter closest to a boundary in units of the gradient). We remove the smallest one from the set of free indices and call it $b$. We define $\\Delta t = t_b - 0$ for this first segment, and this corresponds to the distance we can travel in $t$ till we get to the next boundary (not the one we just explored). As long as the distance to the minimum of the current segment is less than the distance to the next segment ($\\Delta t$) and we still have unexplored segments, we keep going.\nThe update step for each new segment becomes a bit more complicated, since we are now displaced from the start of the algorithm at some point along the piecewise-linear path. We define a point $z_b$ as the distance along parameter $b$ from the starting point to the boundary. We update $\\vec{c} \\to \\vec{c} + \\Delta t \\vec{p}$ and update the function derivatives according to\n$$ f’ = f’ + \\Delta t f’’ + g_b^2 + \\theta g_b z_b - g_b \\vec{(w_b)}^{\\intercal}M\\vec{c} $$\nand\n$$ f’’ = f’’ - \\theta g_b^2 - 2g_b \\vec{(w_b)}^{\\intercal}M\\vec{p} - g_b^2 \\vec{(w_b)}^{\\intercal}M \\vec{(w_b)} $$\nHere we define $\\vec{(w_b)}$ as the $b$th row of the $W_k$ matrix, and we use the previous values of $f’$ and $f’’$ to calculate their updates. Then we update $\\vec{p} \\to \\vec{p} + g_b \\vec{(w_b)}$ and we set the $b$th element of $\\vec{d}$ to zero (this is why d is mutable in the code above) since we are essentially saying that this parameter is no longer part of the set of free parameters. We then repeat the same process as before to find the minimum along this segment, we remove the next free index, and explore the following segment in that direction as long as it is smaller than the distance we just traveled along the current segment. Notice that if a parameter is unbounded or very far away from a bound, this condition cannot be met, so the algorithm will terminate.\nAfter we have explored all viable segments, we calculate the generalized Cauchy point as\n$$ x_i^c = x_i + t_{\\text{tot}} d_i $$\nfor all $i$ with $t_i \\geq t$ (all remaining segments) where $t_{\\text{tot}}$ is the total distance traveled along the path. We do one final update to $\\vec{c}$ before continuing to the next step, subspace minimization.\nThe full code looks something like this:\n\nSubspace Minimization with the Primal Direct Method\nThe authors of [1] list three different ways to actually minimize the function after finding the generalized Cauchy point, the direct primal method, a method using conjugate gradients, and a dual method. We will only go over the direct primal method, since it’s the most common implementation in practice and the method I use in my own implementation.\nThe details of this algorithm are quite daunting, but the general idea is that we can build a “reduced” Hessian of the quadratic approximate $m_k(x)$ which only acts on parameters which are not already at a boundary and then carry out a minimization over the subspace of free parameters. To do this, we start by constructing a matrix $Z_k$ which has dimensions $n\\times n_\\text{free}$ where\n$$ (Z_k)_{i,j} = \\begin{cases} 1 &amp; \\text{if } i = j \\text{ and } j\\in\\mathcal{F} \\\\ 0 &amp; \\text{otherwise} \\end{cases} $$\nWe can then imagine operations $\\hat{B}_k = Z_k^{\\intercal} B_k Z_k$ which take the Hessian as a $n\\times n$ matrix to a reduced Hessian of dimension $n_\\text{free}\\times n_\\text{free}$. We can compute a reduced gradient at the generalized Cauchy point as\n$$ \\hat{r}^c = Z_k^{\\intercal} (g_k + \\theta(x^c - x_k) - W_k M_k \\vec{c}) $$\nWe then want to minimize\n$$ \\hat{m}_k(\\hat{d}) \\equiv \\hat{d}^{\\intercal} \\hat{r}^c + \\frac{1}{2}\\hat{d}^{\\intercal} \\hat{B}_k \\hat{d}$$\nsubject to $l_i - x_i^c \\leq \\hat{d}_i \\leq u_i - x_i^c$ for $i\\in\\mathcal{F}$.\nIt can be shown (though I will certainly not show the details) that\n$$ \\hat{d}^u = -\\left(\\frac{1}{\\theta}\\hat{r}^c + \\frac{1}{\\theta^2}Z^{\\intercal} W \\left( I - \\frac{1}{\\theta} MW^{\\intercal}ZZ^{\\intercal}W \\right)^{-1} MW^{\\intercal}Z\\hat{r}^c\\right) $$\nis the direction in the subspace which minimizes $\\hat{m}_k$. Note the leading minus sign! This cannot be found in equation 5.11 or step 6 of the direct primal method algorithm in [2], but I believe this is a typo. This minus sign is correct, and I’ve confirmed this by looking at several implementations of this algorithm in several different languages. It is also correct within the paper itself, as can be seen in equation 5.7, which states $\\hat{d}^u = -\\hat{B}_k^{-1}\\hat{r}^c$, combined with equation 5.10 which gives the definition for $\\hat{B}^{-1}$. This was absolutely painful to debug, and I hope that if anyone feels the need to go through this paper again, they’ll see this and not have the same issues I did when I first coded this up.\nThe code for this step looks like:\n\nThe rest of the algorithm is fairly similar to the BFGS algorithm. We get a search direction, which is determined either by x_bar - x (or x_cp - x if there are no remaining free indices, which is why we don’t just return the search direction immediately), then we do a line search along this direction, then we move and check if the termination conditions are met. We obtain a new update for $\\vec{s}_k$ and $\\vec{y}_k$ and get rid of the oldest one if the new one meets the curvature condition, and repeat!\nThe link to the full implementation is here. Note that this is a permalink to the most recent commit at time of writing, but I plan to make a rather large update to the API which will slightly alter some of the structure of the algorithm.\nThere are likely several places where I could optimize the code, but I haven’t really gotten to that yet. As far as I know, this is the first and possibly only pure Rust implementation of the L-BFGS-B algorithm (if someone else has a better claim, I’ll recant), and there is probably some room for improvement. If you find something that could be optimized, let me know, make a PR, send me an email!\nThis is the last post in my first series of blog posts on this site. As you might be able to tell, I’m still getting used to writing regularly, and I’ve been a bit preoccupied with my PhD thesis lately, but I hope to have more interesting posts in the future (which will hopefully be written with a bit more forethought than these have been). I hope anyone reading this has enjoyed it, and I hope it clears up some of the general ideas of each algorithm. If anyone is interested in a more in-depth review, check out the cited papers here or any of the citations in the previous posts, as they are full of interesting notes and insights.\n\n\nR. H. Byrd, J. Nocedal, and R. B. Schnabel, “Representations of quasi-Newton matrices and their use in limited memory methods,” Mathematical Programming, vol. 63, no. 1–3, pp. 129–156, Jan. 1994, doi: 10.1007/bf01582063.\n ↩ ↩2\n\n\nR. H. Byrd, P. Lu, J. Nocedal, and C. Zhu, “A Limited Memory Algorithm for Bound Constrained Optimization,” SIAM J. Sci. Comput., vol. 16, no. 5, pp. 1190–1208, Sep. 1995, doi: 10.1137/0916069. ↩ ↩2 ↩3\n\n\n","path":null},{"url":"https://denehoffman.com/blog/tzigane/","title":"Tzigane","description":null,"body":"\n\nI was given an opportunity to perform Ravel’s Tzigane with the All University Orchestra in the fall of 2024 after winning the previous year’s concerto competition. It took a bit to get the microphone audio, but I finally have a complete recording!\n","path":null},{"url":"https://denehoffman.com/blog/the-bfgs-algorithm-family-in-rust-part-2/","title":"The BFGS Algorithm Family in Rust (Part 2)","description":null,"body":"\n\t\n\t\tNote\n\tSince writing the previous post, I have made several simplifications to the library that will change how some structs and traits are used here. The biggest change is that I’ve abandoned generic floats in favor of a feature gate:\n\nWith this code, I can turn the f32 version of my crate on and off with a feature flag. While generics can be nice, in this instance they were actually causing the crate to be slower (just according to some of my own internal benchmarks, this might not be the case in general) and more difficult to read. Any instances in the last post where there was a generic type T representing floating-point values have been replaced with this Float type, so Status&lt;T&gt; is now just Status, for example.\n\n\nLet’s talk about the core BFGS algorithm. The details I’m using here can be found in Nocedal and Wright’s book “Numerical Optimization” in Chapter 6: Quasi-Newton Methods. In the last post, I alluded to this and described gradient descent, but we should really cover what Newton’s method is first.\nNewton’s Method\nThe core idea of gradient descent was that we get the gradient at the initial point and walk downhill. We can couple this with a line search to figure out the optimal step size to take downhill, but that’s about the best we can do with a single derivative. With two derivatives, however, we can do just a bit better (converge faster). First, let’s Taylor expand our function around some point $x$:\n$$ f(x + t) \\approx f(x) + f’(x)t + \\frac{1}{2}f’’(x)t^2 $$\nWe can imagine this as a function of just $t$, keeping $x$ fixed, and try to figure out the value of $t$ that minimizes the function. This can be done by realizing that the derivative of $f(x + t)$ with respect to $t$ should be zero at extrema and the second derivative should be positive at a minimum:\n$$ 0 = \\frac{\\text{d}}{\\text{d}t} \\left(f(x) + f’(x)t  \\frac{1}{2} f’‘(x)t^2 \\right) = f’(x) + f’’(x)t $$\nor $t = -\\frac{f’(x)}{f’’(x)}$.\nIn more than one dimension $n$, we write $f’(x) \\to \\vec{\\nabla}f(\\vec{x})$ (a $n$-dimensional vector) and $f’’(x) \\to \\nabla^2 f(\\vec{x}) = H$. This second derivative term is an $n \\times n$ matrix called the Hessian, and the matrix equivalent of the reciprocal here is the matrix inverse. The Hessian is just a matrix of second derivatives, $H_{ij} = \\frac{\\partial^2 f(\\vec{x})}{\\partial \\vec{x}_i \\partial \\vec{x}_j}$, so it is symmetric by definition.\nThe Hessian is also positive definite, which just means that for all nonzero vectors $\\vec{q}$,\n$$\\vec{q}^{\\intercal} H \\vec{q} &gt; 0$$\nWith all of this in mind, recall that the gradient descent update step was\n$$ \\vec{x}_{k+1} = \\vec{x}_k - \\alpha_k \\vec{\\nabla} f(\\vec{x}_k) $$\nso the equivalent second-derivative (Newton) update step is\n$$ \\vec{x}_{k+1} = \\vec{x}_k - \\alpha_k H_k^{-1} \\vec{\\nabla} f(\\vec{x}_k) $$\nAs far as I know, this is the optimal second-order method for numeric optimization (in theory). In practice, it tends to be horribly inefficient to compute a Hessian matrix and subsequently invert it, and this problem gets worse when you increase the number of free parameters. Particularly, we rarely know the analytic first and second derivatives of our objective function (and it could be argued that knowing those would often make optimization trivial). In most cases, we would have to do these derivatives numerically:\n\nI’m not sure if I’ve written this function as efficiently as possible, but already we can see the issue. To accurately calculate the Hessian, we need to take the gradient in two places for each free parameter (g_plus and g_minus), a process which requires at least two function calls for each free parameter. That’s a total of $4n^2$ function evaluations for $n$ free parameters. If the function takes one second to evaluate and has ten free parameters, the gradient will take twenty seconds, but the Hessian will take over six minutes. Now it might seem silly to speculate on a function which takes a full second to evaluate (at that point, we can expect any optimization problem is going to take a while), but the real issue is scaling. If we add a single free parameter to this problem, it now takes eight minutes. Most of the problems I deal with in my research can be evaluated in less than 200ms, but they contain over 40 free parameters. Do the math, that’s 21 minutes for a single evaluation of the Hessian, and we have to do that at every step in the optimization!\nFortunately, there’s a better, faster way. Rather than calculating the full Hessian, let’s just approximate it! After all, in the limit of very small steps, the Hessian shouldn’t be changing that much (for nice functions), so it makes sense that there should be some way to approximate the next Hessian given the current (and the gradients at both points). This is what puts the “quasi” in quasi-Newton methods. We outline some way of approximating the current Hessian given some history of past Hessian approximates and gradients and apply Newton’s method for optimization.\nLet’s refer to this approximate Hessian as $B_k$[1]. Furthermore, following the derivation in Nocedal and Wright, let’s write the second-order Taylor expansion of our function at the point $\\vec{x}_k$. I’ll use $f_k \\equiv f(\\vec{x}_k)$ to represent the objective function and $\\vec{g}_k \\equiv \\vec{\\nabla}f(\\vec{x}_k)$ to represent the gradient.\n$$\nm_k(\\vec{p}) = f_k + \\vec{g}_k^{\\intercal} \\vec{p} + \\frac{1}{2} \\vec{p}^{\\intercal} B_k \\vec{p}\n$$\nThis approximation is identical to our function when $\\vec{p} = 0$, and furthermore, the gradient is also the same. The only difference is in the second derivatives, which are approximately the same. We can assume that we’ll use some line search to find the optimal step length $\\alpha_k$ to get to the new point, $\\vec{x}_{k+1} = \\vec{x}_k + \\alpha_k \\vec{p}_k$. Let’s go to this next point and write out the Taylor expansion for the next step here:\n$$\nm_{k+1}(\\vec{p}) = f_{k+1} + \\vec{g}_{k+1}^{\\intercal} \\vec{p} + \\frac{1}{2} \\vec{p}^{\\intercal} B_{k+1} \\vec{p}\n$$\nNow imagine we took this step and looked back at the old position and calculated the gradient using this new expansion evaluated at the old position. We would hope to get the same result as when we used our old expansion! We can actually make this a constraint on our matrix $B$. To look back at the old position, we evaluate the function at $p = -\\alpha_k\\vec{p}_k$, since this is the step we just took but reversed in direction. We can then write the gradient as\n$$\n\\vec{\\nabla} m_{k+1}(-\\alpha_k\\vec{p}_k) = \\vec{g}_{k+1} - \\alpha_k B_{k+1} \\vec{p}_k\n$$\nand with the requirement that $ \\vec{\\nabla} m_{k+1}(-\\alpha_k\\vec{p}_k) = \\vec{g}_k $, we get a new rule for our approximate Hessian:\n$$\nB_{k+1}\\alpha_k\\vec{p}_k = \\vec{g}_{k+1} - \\vec{g}_k\n$$\nThere is some conventional notation to this that we will use throughout the derivation. Rather than using each individual gradient and position, it is helpful to just think about the change in gradient and position, or\n$$ \\vec{s}_k = \\vec{x}_{k+1} - \\vec{x}_k = \\alpha_k\\vec{p}_k $$\nand\n$$ \\vec{y}_k = \\vec{g}_{k+1} - \\vec{g}_k $$\nIn this notation, the gradient requirement (called the secant equation), is as follows:\n$$ B_{k+1}\\vec{s}_k = \\vec{y}_k $$\nRemember when we said the Hessian is positive definite? We would like our Hessian approximate to be positive definite too. Since we don’t include any zero-length jumps, $|\\vec{s}_k| &gt; 0$, so we can multiply the previous equation by another $\\vec{s}_k$ to find:\n$$ \\vec{s}_k^{\\intercal} B_{k+1} \\vec{s}_k &gt; 0 \\implies \\vec{s}_k^{\\intercal} \\vec{y}_k &gt; 0 $$\nThis requirement on $\\vec{s}_k$ and $\\vec{y}_k$ is called the curvature condition. As it turns out, there’s a fairly simple way to ensure this using the Wolfe condition from my previous article. Recall that during our line search, we required that\n$$\\begin{align}\\vec{p}_k^{\\intercal} \\vec{g}_{k+1} &amp;\\geq c_2(\\vec{p}_k^{\\intercal} \\vec{g}_k) \\\\ \\vec{s}_k^{\\intercal} \\vec{g}_{k+1} &amp;\\geq c_2(\\vec{s}_k^{\\intercal} \\vec{g}_k) \\\\ \\vec{s}_k^{\\intercal} (\\vec{y}_k + \\vec{g}_{k}) &amp;\\geq c_2(\\vec{s}_k^{\\intercal} \\vec{g}_k) \\\\ \\vec{s}_k^{\\intercal} \\vec{y}_k + \\vec{s}_k^{\\intercal} \\vec{g}_{k} &amp;\\geq c_2(\\vec{s}_k^{\\intercal} \\vec{g}_k) \\\\ \\vec{s}_k^{\\intercal} \\vec{y}_k &amp;\\geq (c_2 - 1)(\\vec{s}_k^{\\intercal} \\vec{g}_k) \\\\ \\vec{s}_k^{\\intercal} \\vec{y}_k &amp;\\geq (c_2 - 1)\\alpha_k \\vec{p}_k \\vec{g}_k \\end{align}$$\nSince we require $c_2 &gt; 1$ and $\\alpha_k &gt; 0$, and assuming $\\vec{p}_k$ is somewhat aligned with the gradient (or at a minimum, $\\vec{p}_k$ is pointing somewhere downhill), then the curvature condition clearly holds.\nUnfortunately, we’re only a bit better off than we were when we had no idea what $B$ was. While these conditions limit the space of possible $B$ matrices, the set is still infinite. We need to find more ways to narrow down the space to a unique solution.\nOne way we could approach this is by assuming that $B$ shouldn’t change too much at each update. There are a number of ways of going about this, but the basic idea is that we choose some norm on the change of the matrix between updates with different norms leading to different quasi-Newton algorithms. In particular, minimizing the Frobenius norm for this particular problem will give us the Davidon-Fletcher-Powell (DFP) algorithm. It has a nice update step which we can apply to $B$ that looks like this:\n$$B_{k+1} = (I - \\rho_k \\vec{y}_k \\vec{s}_k^{\\intercal}) B_k (I - \\rho_k \\vec{s}_k \\vec{y}_k^{\\intercal}) + \\rho_k \\vec{y}_k \\vec{y}_k^{\\intercal}$$\nwhere $\\rho_k = \\frac{1}{\\vec{y}_k^{\\intercal} \\vec{s}_k}$. However, we aren’t actually done yet, since in the quasi-Newton update step, we really need the matrix $B_{k+1}^{-1}$. We could go and calculate a matrix inverse at each step, but it turns out we don’t need to thanks to the Sherman-Morrison-Woodbury formula, which says that the inverse of a rank-$k$ correction on a matrix is equivalent to a rank-$k$ correction on the inverse, or more precisely,\n$$(B + UCV)^{-1} = B^{-1} - B^{-1}U(C^{-1}+VB^{-1}U)^{-1}VB^{-1}$$\nwhere $B$ is $n\\times n$, $C$ is $k\\times k$, $U$ is $n\\times k$, and $V$ is $k\\times n$. This is a tough one, but we should take the time to work it out. First we expand the right side of the DFP formula:\n$$\nB_k - \\rho_k \\vec{y}_k \\vec{s}_k^{\\intercal} B_k - \\rho_k B_k \\vec{s}_k \\vec{y}_k^{\\intercal} + \\rho_k^2 \\vec{y}_k \\vec{s}_k^{\\intercal} B_k \\vec{s}_k \\vec{y}_k^{\\intercal} + \\rho_k \\vec{y}_k \\vec{y}_k^{\\intercal}\n$$\nEverything except for the very first $B_k$ should be thought of as those matrices $U$, $V$, and $C$. It is also helpful to know that by a rank-$k$ correction, we mean that the correction is written as $A’ = A + \\vec{u}_1\\vec{v}_1^{\\intercal} + \\vec{u}_2\\vec{v}_2^{\\intercal} + … + \\vec{u}_k\\vec{v}_k^{\\intercal}$. The two terms in the original DFP formula form such a correction of rank-2, since the first terms involve both $\\vec{y}_k$ and $\\vec{s}_k$ scaled by $\\rho_k$ and $B_k$ while the second term only involves $\\vec{y}_k$ scaled by $\\rho_k$, making it linearly independent. We then require $C$ to be a $2\\times 2$ matrix, $U$ to be $n\\times 2$, and $V$ to be $2\\times n$, where $n$ is the dimensionality of the Hessian. It’s a bit tricky to see, but we can write this all in the form:[2]\n$$\nB_{k+1} = B_k + \\begin{bmatrix}\\rho_k\\vec{y}_k &amp; \\rho_k B_k\\vec{s}_k \\end{bmatrix} \\begin{bmatrix}1 + \\rho_k \\vec{s}_k^{\\intercal} B_k \\vec{s}_k &amp; -1 \\\\ -1 &amp; 0 \\end{bmatrix} \\begin{bmatrix} \\vec{y}_k \\\\ \\vec{s}_k B_k \\end{bmatrix}\n$$\nI’ll leave the actual application of the Sherman-Morrison-Woodbury formula as an exercise to the reader, but the conclusion is this rather nice looking update step for the inverse of the (approximate) Hessian:\n$$\nB_{k+1}^{-1} = B_k^{-1} - \\frac{B_k^{-1} \\vec{y}_k \\vec{y}_k^{\\intercal} B_k^{-1}}{\\vec{y}_k^{\\intercal} B_k^{-1} \\vec{y}_k} + \\frac{\\vec{s}_k \\vec{s}_k^{\\intercal}}{\\vec{y}_k^{\\intercal} \\vec{s}_k}\n$$\nAt this point, we should be pretty happy. Given some starting value for the inverse Hessian, we have a way of determining a new inverse Hessian at our next step given the value of the old one! We could go and write this up, but then this article would need a different title. It just so happens that you can apply this very same process, but swap out $B$ for $B^{-1}$ at every step (with a new secant condition, $ B_{k+1}^{-1}\\vec{y}_k = \\vec{s}_k $) and skip that last bit entirely, and it works better in practice! In other words, the update step,\n$$ B_{k+1}^{-1} = (I - \\rho_k \\vec{s}_k \\vec{y}_k^{\\intercal})B_{k}^{-1} (I - \\rho_k \\vec{y}_k \\vec{s}_k^{\\intercal}) + \\rho_k \\vec{s}_k \\vec{s}_k^{\\intercal} $$\nis also a unique solution to our quasi-Newton problem. This is the BFGS update. Of course, we still have the problem of what values should be assigned to $B_0^{-1}$. The neat part about BFGS is that this update step tends to be self-correcting (DFP is generally worse in this regard), assuming the Wolfe condition is met in the line search (this is why I spent so much time on the line search part of the previous post, it really is that important to make educated leaps in the gradient direction!). What this means is that even if we start with a bad initial guess at $B_0^{-1}$, after some number of steps, we should get to a good approximation regardless. We could of course just start with the numerically calculated Hessian, invert it, and go from there, but in practice, this really isn’t worth the effort. Instead, we can just use the identity matrix[3] for the very first step (just regular gradient descent!) and then correct that identity matrix each time. We will later find that this isn’t even the most efficient way of doing things when we look at L-BFGS, but for now we have a formula, let’s implement it!\nLet’s start with a struct to hold our new algorithm:\n\nNext, our update formula:\n\nI’ve modified the Algorithm trait since making the previous post:\n\nWe now just pass the Minimizer’s Status in as a mutable reference and ask the Algorithm to modify it accordingly. The implementation of the BFGS Algorithm is simply:\n\nThat’s pretty much it. Of course, we need to add a few details to tell the algorithm where to stop. Some standard criteria are when the absolute change in the function value or the norm of the gradient goes below some tolerance, and you can see those termination conditions implemented in the full code here.\n\nI have to apologize a bit for the amount of changes in the code that have taken place since the last article. I’m developing this in conjunction with a library I’m currently using for amplitude analysis of particle physics data, and being relatively new to Rust, I often find improvements by realizing that I’ve backed myself into an implementation corner. I hope this article serves as a nice introduction to the BFGS algorithm and how one might implement it in Rust, but it is likely not the best way to implement this or the other algorithms I’ve discussed. If any readers see places where my code could be improved, I would welcome suggestions via the GitHub repo.\n\nWith that, I’ll end this portion of the series. In the next post, I’ll discuss the L-BFGS algorithm, the “limited memory” version of BFGS. The surprising thing we will see is that this method tends to work better than BFGS in most practical settings, and its close, bounded cousin, L-BFGS-B also tends to outperform standard BFGS updates. This is mostly because these methods not only reduce the memory required to perform BFGS updates (for large parameter spaces), they tend to involve fewer operations, which makes them more efficient to calculate. The core idea will be, rather than update and store the entire inverse Hessian, just store the changes and update an identity matrix, and while we’re at it, just calculate the update step directly from the stored changes to the gradient!\n\n\nThis notation is somewhat standard, but if you spend any time reading old papers on this, you’ll find that they tend to use $B$, $H$, and their inverses rather interchangeably. For the sake of clarity, $H$ here will always refer to the true Hessian and $B$ to some approximated Hessian. ↩\n\n\nCredit due to A.Γ. in this post on the Mathematics StackExchange ↩\n\n\nWhile this would work, it’s recommended to actually use $\\frac{\\vec{y}_k^{\\intercal}\\vec{s}_k}{\\vec{y}_k^{\\intercal}\\vec{y}_k}$ for the very first step, after the step has been made but before the first update is performed, and this is reflected in the code. ↩\n\n\n","path":null},{"url":"https://denehoffman.com/blog/the-bfgs-algorithm-family-in-rust-part-1/","title":"The BFGS Algorithm Family in Rust (Part 1)","description":null,"body":"The BFGS (Broyden-Fletcher-Goldfarb-Shanno) algorithm and its derivatives were (and for the most part still are) the gold standard methods for quasi-Newton optimization. In this post, I want to give a brief overview of the main idea, the limited-memory adaptation (L-BFGS), and the bounded version (L-BFGS-B) and how I implemented them in a Rust crate I’m developing called ganesh. The full algorithm can be seen there, and I will mainly be focusing on the main methodology, since the actual literature on it is rather old and difficult to parse (and even has a few typos!).\nThat being said, I wouldn’t have done it without the following articles/projects which have guided my understanding:\n\n“Numerical Optimization” by Jorge Nocedal and Stephen J. Wright\n“Numerical Optimization: Understanding L-BFGS” by Aria Haghighi\nL-BFGS-B in pure Python by @avieira (Alex Vieira?)\nL-BFGS-B in pure MATLAB by Brian Granzow\n“A Limited Memory Algorithm for Bound Constrained Optimization” by Richard H. Byrd, Peihuang Lu, Jorge Nocedal, and Ciyou Zhu\n\nQuasi-Newton?\nTypically, in numeric optimization problems, we want to define some stepping process that will cause us to approach a minimum or maximum as quickly as possible. For our purposes, we’ll just assume we only care about minimizing, and we’ll also assume that there is a single global minimum to the function in question, or at least that we don’t care about falling into a deep local minimum. The simplest non-trivial way to do this is by gradient descent:\n$$\n\\vec{x}_{k+1} = \\vec{x}_k - \\alpha_k \\vec{\\nabla} f(\\vec{x}_k)\n$$\nwhere $f: \\mathbb{R}^n \\to \\mathbb{R}$ is the objective function we are trying to minimize and $\\alpha_n$ is some positive step length (also called the learning rate). The minus sign here is why we call it gradient descent; we are always moving opposite the gradient, which always points uphill. For simplicity, we’ll also refer to the gradient as a function $\\vec{g}(\\vec{x}) \\equiv \\vec{\\nabla}f(\\vec{x})$ Now if you just throw in some very small value for $\\alpha$ and cross your fingers, you might eventually end up at the function’s minimum, but it certainly won’t be the most efficient way to get there. If your $\\alpha$ is too big, you could end up overshooting the minimum and bouncing back and forth around it endlessly.\nThere are several ways we can optimize the choice of step length. Skip ahead to here to see the implementation I use for the BFGS family.\nRust implementation\nFunction trait\nLet’s start by defining a trait which will evaluate our function $f$ and its gradient $\\vec{g}$. I want the input parameter values $\\vec{x}$ to be generic so that both f64 and f32 types can be used, as well as any other struct with the right trait implementations. I also want these functions to return Results with a generic error type E so that users can handle any errors their own functions create. Finally, we should consider adding optional arguments to these functions. Again, we will turn to generics, but allow users to pass a &amp;mut U called user_data. This term is mutable because it will give us the most flexibility later on. The trait for a function might look something like this:\n\nI also use a little macro to convert raw numeric fields to our generic type T, if possible:\n\nLet’s walk through the anatomy of the trait above. I think the evaluate method is pretty self-explanatory (given that it’s an empty template), but the gradient method is a bit more complex. First of all, I’m implementing a central finite-difference here:\n$$\n\\frac{\\partial f(\\vec{x})}{\\partial x_i} = \\frac{f(\\vec{x} + h_i \\hat{e}_i) - f(\\vec{x} - h_i \\hat{e}_i)}{2h_i}\n$$\nThe tricky detail is choosing a value for $h_i$. In practice, machine epsilon is too small! What we actually should use is $h_i = \\sqrt[3]{\\varepsilon} x_i $ when $x_i \\neq 0$ and $h_i = \\sqrt[3]{\\varepsilon}$ in the event that $x_i = 0$.\nAlgorithm Trait\nNext, since we want to implement three algorithms with very similar features, it might make sense to create a generic trait that can be used by some executor that will wrap all of these methods into a nice API. All of these algorithms will will need to know the following:\n\nThe objective Function\nThe starting point $x_0$\nAny bounds on the free parameters (we will ignore bounds for the BFGS and L-BFGS methods here, although an experimental change of variables is implemented in the final crate)\nThe user_data to pass to the Function\n\nWe should also define what an algorithm should give us in return!\n\nThe best position at the end of the minimization, $x_\\text{best}$\nThe function value at that point, $f(x_\\text{best})$\nThe number of function/gradient evaluations\nSome indication as to whether the result of the minimization is valid\nSome String message that can tell us any additional information about how the fit progressed/is progressing\n\nLet’s call this struct Status and define it as follows:\n\nNote that we have set this up in a way that doesn’t let any algorithm decrement the number of function/gradient evaluations. Additionally, no outside function can un-converge a converged Status. We will also typically update $f(x_\\text{best})$ every time we update $x_\\text{best}$, so there’s only one way to do this to ensure they don’t get out of sync for any reason.\nNext, the Algorithm trait itself:\n\nMost of these methods are fairly self-explanatory and have very similar signatures. Finally, let’s wrap all of this up in a nice interface for the end-user to work with:\n\nFor now, we will ignore the Bound struct mentioned here, since we won’t use it till we get to the L-BFGS-B algorithm. Note that PhantomData is required here because we don’t actually store anything of type U or E but we need to include it in generics.\nThe main minimize function should also look pretty straightforward. We first call Algorithm::initialize and then proceed into a while-loop that checks if we either exceed the maximum allowed algorithm steps or if Algorithm::check_for_termination tells us to stop the algorithm (in case of problems or convergence). Inside this loop, we just run Algorithm::step. We finish off with Algorithm::postprocessing and grab the final Status of the algorithm. This will be the workflow for every Algorithm we implement[1].\nLine Searches\nWe will be implementing an algorithm that attempts to satisfy the Strong Wolfe conditions. These are conditions for accepting a step length given some step direction $\\vec{p}$ (we’ll see later why we need to generalize this, but for now you can always just imagine $\\vec{p} = -g(\\vec{x})$).\nThe first of these conditions is also called the Armijo rule:\n$$\nf(\\vec{x} + \\alpha_k \\vec{p}) \\leq f(\\vec{x}) + c_1 \\alpha_k \\left(\\vec{p} \\cdot \\vec{g}(\\vec{x})\\right)\n$$\nfor some value $0 &lt; c_1 &lt; 1$. The usual choice of $c_1$ is $10^{-4}$, which I believe just comes from some experimentation on standard test functions. This method is also called the sufficient decrease condition, and we can see why. The left-hand side is the function value at the new location, which we hope is at least smaller than the previous location (otherwise we are ascending!). However, for it to be sufficiently smaller, the difference must exceed the final term in the equation, which is usually going to be negative due to that dot product.\nThe second condition, dubbed the curvature condition, requires that the gradient of the function decrease sufficiently. This is usually harder to accomplish, so when we implement this in Rust, we will make it optional but desired.\n$$\n-\\left(\\vec{p} \\cdot \\vec{g}(\\vec{x} + \\alpha_k \\vec{p})\\right) \\leq -c_2 \\left(\\vec{p} \\cdot \\vec{g}(\\vec{x})\\right)\n$$\nThis condition adds another hyperparameter, $0 &lt; c_1 &lt; c_2 &lt; 1$ where $c_2 = 0.9$ in most applications. However, if we really want to find the best point, we should try to satisfy the strong version of the curvature condition:\n$$\n\\left|\\vec{p}_k \\cdot \\vec{g}(\\vec{x}_k + \\alpha_k \\vec{p}_k)\\right| \\leq c_2 \\left|\\vec{p}_k \\cdot \\vec{g}(\\vec{x}_k)\\right|\n$$\nWe’ll start the implementation with another trait (since other algorithms might use a different search method):\n\nWe’ll be implementing Algorithms 3.5 and 3.6 from “Numerical Optimization”, which (roughly) reads as follows:\nAlgorithm 3.5\n\n$\\alpha_0 \\gets 0$, $\\alpha_\\text{max} &gt; 0$, $\\alpha_1 \\in (0, \\alpha_\\text{max})$, $i \\gets 1$\nloop\n\n\nif\n$f(\\vec{x} + \\alpha_i \\vec{p}) &gt; f(\\vec{x}) + c_1\\alpha_i\\left(\\vec{p}\\cdot\\vec{g}(\\vec{x})\\right)$ (not Armijo)\nor\n($i &gt; 1$ and $f(\\vec{x} + \\alpha_i \\vec{p}) \\geq f(\\vec{x} + \\alpha_{i-1}\\vec{p})$) (the function value has not decreased since the previous step)\nthen return $\\text{zoom}(\\alpha_{i-1}, \\alpha_i)$\n\n\nif\n$\\left|\\vec{p}\\cdot\\vec{g}(\\vec{x} + \\alpha_i\\vec{p})\\right| &lt; c_2 \\left|\\vec{p}\\cdot\\vec{g}(\\vec{x})\\right|$ (strong Wolfe)\nthen return $\\alpha_i$\n\n\nif\n$\\vec{p}\\cdot\\vec{g}(\\vec{x} + \\alpha_i\\vec{p}) \\geq 0$ (gradient at new position generally points in the same direction as the given step direction)\nthen return $\\text{zoom}(\\alpha_i,\\alpha_{i-1})$\n\n\n$\\alpha_{i+1} \\in (\\alpha_i, \\alpha_\\text{max})$ (choose some larger step that is smaller than the max step)\n\n\n$i \\gets i + 1$\n\n\n\n\nIn each loop, we are first checking to see if the function is sufficiently decreasing. If it isn’t, we know that the step size overshoots. Imagine we are just minimizing into a 1D parabola. If we are sitting to the left of the minimum, the optimal step length $\\alpha_\\text{opt}$ would put us right at the minimum. If we pick a step $\\alpha &lt; \\alpha_\\text{opt}$ would be fine, but it would mean we converge slower than optimal. The same can be said for a step length $\\alpha &gt; \\alpha_\\text{opt}$, but at a certain point, we will be stepping to a point higher up the parabola than where we began, even though we are moving in the right direction! Step 2.1 ensures that if this happens, we will do a more refined search (zoom) between the current and previous step lengths (note that $\\alpha_{i-1} = 0$ when $i=1$ on the first loop). This will happen a lot if we pick a starting step length that is too large (see the following diagram).\n\nIf we are decreasing, we are in that region where we are converging, but we might not be converging at an optimal rate. This is where the strong Wolfe condition comes in. We first project the gradients at the original and stepped positions onto the step direction. If the gradient is the step direction (well, opposite to it), then we can ignore $\\vec{p}$ here and think of this in terms of a change in gradient magnitude. If the gradient decreases by at least a factor of $c_2$, we accept the step (see the following diagram).\n\nIf we are decreasing sufficiently, but the magnitude of the projected gradient isn’t (the gray region in the previous plots), we are either undershooting the optimal step, in which case we should increase the step size and run the loop again (Step 4) or we are overshooting, in which case we should run zoom between the current step and the previous one. How do we tell? Well, if we overshoot, the gradient on the next step will tell us to move in the opposite direction, but if we undershoot, we should still be moving in the same direction. This is what the if-statement of Step 3. checks for.\n\nIn the above plot, both points meet the Armijo condition, but they fail to meet the strong Wolfe condition. This means they make it to Step 3 in our line search algorithm. For the left-most point, the gradient points in the $-x$ direction while the step was in the $+x$ direction, so the condition at Step 3 is not satisfied, and we increase our step size (hopefully landing in the green region). For the right-most point, the gradient now points in the $+x$ direction, so Step 3 is satisfied and we again zoom between this step size and the previous. Note that the arguments to zoom are switched here. In the definition of the zoom algorithm, we will refer to the first argument as $\\alpha_{\\text{lo}}$ and the second as $\\alpha_{\\text{hi}}$. However, since $\\alpha_i$ is strictly increasing in Algorithm 3.5, we shouldn’t think of one of these values as larger than the other, but rather that the function evaluations at these points are lower or higher. Here, $\\alpha_\\text{lo}$ will always refer to a step length which satisfied the Armijo condition, which means that the function value with this step length is the lower of the two. $\\alpha_\\text{hi}$ will be the previous step if the current one is the first to satisfy the Armijo condition (Step 2.3) or it will be the current step if the previous step gave a smaller function evaluation (Step 2.1). In either case, we know that the optimal step length is in the given range.\nAlgorithm 3.6 (zoom)\n\nloop\n\n\nChoose $\\alpha_j$ between $\\alpha_{\\text{lo}}$ and $\\alpha_{\\text{hi}}$ (it’s possible for $\\alpha_\\text{lo} &gt; \\alpha_\\text{hi}$).\n\n\nif\n$f(\\vec{x} + \\alpha_j \\vec{p}) &gt; f(\\vec{x}) + c_1\\alpha_j\\left(\\vec{p}\\cdot\\vec{g}(\\vec{x})\\right)$ (not Armijo)\nor\n$f(\\vec{x} + \\alpha_j \\vec{p}) \\geq f(\\vec{x} + \\alpha_{\\text{lo}}\\vec{p})$ (the function value has not decreased relative to $\\alpha_{\\text{lo}}$)\nthen $\\alpha_\\text{hi} \\gets \\alpha_j$\nelse\n\n\nif\n$\\left|\\vec{p}\\cdot\\vec{g}(\\vec{x} + \\alpha_j\\vec{p})\\right| \\leq c_2\\left|\\vec{p}\\cdot\\vec{g}(\\vec{x})\\right|$\nthen return $\\alpha_j$\n\n\nif\n$\\vec{p}\\cdot\\vec{g}(\\vec{x} + \\alpha_j\\vec{p}) (\\alpha_\\text{hi} - \\alpha_\\text{lo}) \\geq 0$\nthen $\\alpha_\\text{hi} \\gets \\alpha_\\text{lo}$\n\n\n$\\alpha_\\text{lo} \\gets \\alpha_j$\n\n\n\n\n\n\nThere are only a few possible outcomes of this loop. The “middle” outcome is to return $\\alpha_j$ if it satisfies the Strong Wolfe and Armijo conditions. We check first for Armijo (remember, this is generally less restrictive), and if it’s not satisfied, or if the evaluation is worse than $\\alpha_{\\text{lo}}$, we move $\\alpha_{\\text{hi}}$ to $\\alpha_j$. Remember, the subscripts represent the relative value of the function evaluated at that step, and we know that Algorithm 3.5 will guarantee that the two steps given will surround the “green” region of optimal step size. If Armijo is satisfied but Wolfe is not, we will always move $\\alpha_{\\text{lo}}$ up to $\\alpha_j$, but if the condition in 2.2 is met, this implies that either the order of $\\alpha_{\\text{hi}}$ and $\\alpha_{\\text{lo}}$ is opposite what we think it should be (because in reality these functions are not always smooth minima like the previous diagrams) or the step $\\alpha_j$ no longer goes in the direction of steepest descent. In either case, we then need move our $\\alpha_{\\text{hi}}$ endpoint to $\\alpha_{\\text{lo}}$ first. I’d recommend drawing out several minima scenarios to get a handle on this algorithm, but eventually it will start to make some sense.\nTogether, these algorithms constitute a line search which should result in a step that satisfies Strong Wolfe curvature conditions. This is needed to get optimal convergence from BFGS-like algorithms, but in practice it’s not always efficient to run either algorithm in a (possibly infinite) loop, so I add a maximum number of loops to both algorithms. From my own testing, most problems will have no issue converging within a maximum of 100 iterations for each algorithm. That being said, here’s my implementation of the line search:\n\nThe full implementation (with a nicer API and some other features I’m not going to mention in these blog posts) can be found here. In the next post, I will describe the first of the BFGS family of algorithms, the standard BFGS algorithm (no bounds, no limited-memory optimizations).\n\n\nIn the full code, there are additional clauses for updating outside Observers, which can monitor the Algorithm at each step. ↩\n\n\n","path":null},{"url":"https://denehoffman.com/","title":"","description":null,"body":"Dene Hoffman\n\n  \n\nPhysics PhD Candidate studying glueballs at GlueX\nI’m a recent physics PhD, formerly working at Carnegie Mellon University studying the strong force through the GlueX collaboration. GlueX is a multinational collaboration located in Hall D at Jefferson Lab which collides high-energy photons with a proton target.\nThe main goal of GlueX is a search for particles with exotic quark content. Standard composite particles are identified in two main categories, mesons (made of a quark-antiquark pair) and baryons (three quarks). However, there has been recent experimental evidence of states with more than three quarks, dubbed tetraquarks, pentaquarks, etc. Computer simulations (Lattice QCD) predict additional states such as glueballs, which contain no quarks at all and are just bound states of the “gluons” that hold matter together, and hybrid mesons, where a valence gluon contributes to the total angular momentum to produce “forbidden” quantum numbers.\nMy thesis work consists of a study of $K_SK_S$ (pairs of K-short mesons) photoproduction. This gives us access to even-spin $f$ and $a$ mesons (light, flavorless particles with isospin $0$ and $1$ respectively), the former of which are interesting for several reasons. First, they share many of the same quantum numbers, the values we use to classify particles, with glueballs, hypothetical particles that contain only gluons, the force-carrier of the strong interaction. The lightest of these glueballs is predicted to look nearly identical to some of the $f_0$ mesons (spin-$0$ $f$ mesons), and it turns out that there are too many of these $f_0$ mesons seen in experiments for them to all be compatible with the quark model.\nHowever, the downside of this particular study is that there are lots of other particles present in the $K_SK_S$ channel, and many of them overlap each other. Hopefully my work at GlueX can provide a small step in disentangling this complex set of states and move us closer to understanding the complex physics of quantum chromodynamics. The entire thesis, along with the analysis code and results (including some not included in the thesis) can be found here. The thesis is also available on CMU’s thesis repository.\nI recently defended my thesis and am now focused on some software projects related to that work, particularly ganesh, an optimization library written in pure Rust and laddu, an amplitude analysis library for Python and Rust. I hope to start a postdoc in the spring, but until then I am open to remote work.\nLinks\n\nEmail: dene at cmu dot edu dot saola (my email doesn’t really end with a spindlehorn)\nORCID\nGoogle Scholar\nGitHub\n\n","path":null},{"url":"https://denehoffman.com/publications/","title":"Publications","description":null,"body":"2023\nFano Resonant Optical coatings platform for Full Gamut and High Purity Structural Colors\nMohamed ElKabbash, Nathaniel Hoffman, Andrew Lininger, Sohail Jalil, Theodore Letsou, Michael Hinczewski, Giuseppe Strangi, Chunlei Guo\n\n  Abstract\nStructural coloring is a photostable and environmentally friendly coloring approach that harnesses optical interference and Nanophotonic resonances to obtain colors with a range of applications including display technologies, colorful solar panels, steganography, décor, data storage, and anticounterfeiting measures. We show that optical coatings exhibiting the photonic Fano Resonance present an ideal platform for structural coloring-they provide full color access, high color purity, high brightness, controlled iridescence, and scalable manufacturing. We show that an additional oxide film deposited on Fano resonant optical coatings (FROCs) increases the color purity (up to 97%) and color gamut coverage range (&gt; 99% coverage of the sRGB and Adobe color spaces). For coloring applications that do not require high spatial resolution, FROCs have a significant advantage over existing structural coloring schemes.\n\n2021\nCluster variation method analysis of correlations and entropy in BCC solid solutions\n(arXiv:2007.13219)\nNathaniel Hoffman, Michael Widom\n\n  Abstract\nSolid solutions occur when multiple chemical species share sites of a common crystal lattice. Although the single site occupation is random, chemical interaction preferences bias the occupation probabilities of neighboring sites, and this bias reduced the entropy of mixing below its ideal value. Sufficiently strong bias leads to symmetry-breaking phase transitions. We apply the cluster variation method to explore solid solutions on body centered cubic lattices in the context of two specific compounds that exhibit opposite ordering trends. Employing density functional theory to model the energetics, we show that CuZn exhibits an order-disorder transition to the CsCl prototype structure, while AlLi instead takes the NaTl prototype structure, and we evaluate their temperature-dependent order parameters, correlations and entropies.\n\nFano-resonant ultrathin film optical coatings\nMohamed ElKabbash, Theodore Letsou, Sohail A Jalil, Nathaniel Hoffman, Jihua Zhang, James Rutledge, Andrew R Lininger, Chun-Hao Fann, Michael Hinczewski, Giuseppe Strangi, Chunlei Guo\n\n  Abstract\nOptical coatings are integral components of virtually every optical instrument. However, despite being a century-old technology, there are only a handful of optical coating types. Here, we introduce a type of optical coatings that exhibit photonic Fano resonance, or a Fano-resonant optical coating (FROC). We expand the coupled mechanical oscillator description of Fano resonance to thin-film nanocavities. Using FROCs with thicknesses in the order of 300 nm, we experimentally obtained narrowband reflection akin to low-index-contrast dielectric Bragg mirrors and achieved control over the reflection iridescence. We observed that semi-transparent FROCs can transmit and reflect the same colour as a beam splitter filter, a property that cannot be realized through conventional optical coatings. Finally, FROCs can spectrally and spatially separate the thermal and photovoltaic bands of the solar spectrum, presenting a possible solution to the dispatchability problem in photovoltaics, that is, the inability to dispatch solar energy on demand. Our solar thermal device exhibited power generation of up to 50% and low photovoltaic cell temperatures (~30 °C), which could lead to a six-fold increase in the photovoltaic cell lifetime.\n\n2020\nUltrathin-film optical coating for angle-independent remote hydrogen sensing\nMohamed ElKabbash, Kandammathe Valiyaveedu Sreekanth, Arwa Fraiwan, Jonathan Cole, Yunus Alapan, Theodore Letsou, Nathaniel Hoffman, Chunlei Guo, R Mohan Sankaran, Umut A Gurkan, Michael Hinczewski, Giuseppe Strangi\n\n  Abstract\nWe demonstrated an optically-active antireflection, light absorbing, optical coating as a hydrogen gas sensor. The optical coating consists of an ultrathin 20 nm thick palladium film on a 60 nm thick germanium layer. The ultrathin thickness of the Pd film (20 nm) mitigates mechanical deformation and leads to robust operation. The measurable quantities of the sensors are the shift in the reflection minimum and the change in the full width at half maximum of the reflection spectrum as a function of hydrogen gas concentration. At a hydrogen gas concentration of 4%, the reflection minimum shifted by ∼46 nm and the FWHM increased by ∼228 nm. The sensor showed excellent sensitivity, demonstrating a 6.5 nm wavelength shift for 0.7% hydrogen concentration, which is a significant improvement over other nanophotonic hydrogen sensing methods. Although the sensor's response showed hysteresis after cycling hydrogen exposure, the sensor is robust and showed no deterioration in its optical response after hydrogen deintercalation.\n\n2017\nTunable black gold: controlling the near‐field coupling of immobilized au nanoparticles embedded in mesoporous silica capsules\nMohamed ElKabbash, Ana Sousa‐Castillo, Quang Nguyen, Rosalia Mariño‐Fernández, Nathaniel Hoffman, Miguel A Correa‐Duarte, Giuseppe Strangi\n\n  Abstract\nEfficient light-to-heat conversion is central for various applications such as thermo-photovoltaics and solar steam generation. Although metals can strongly absorb light and generate heat, their free electrons shield the electric field before any substantial penetration in the metal. Excitation of surface plasmons can suppress metal reflection and convert it into a black metal, for example, black gold. In this work, mesoporous silica capsules grafted with immobilized Au nanoparticles (NPs) with different sizes via controlled chemical synthesis are synthesized. It is shown that changing the size of immobilized NPs modifies the interparticle coupling strength, thus, modifying the NPs absorption. The broadness of the plasmon resonance is tuned across the visible, near-infrared, and short wavelength infrared regions. The ability to control the broadness of black gold absorption is not possible in other systems based on bottom-up synthesis. The proposed approach broadens the possibilities of utilizing black gold in many applications such as thermo-photovoltaics, and solar energy harvesting especially in hybrid solar converters.\n\nIridescence-free and narrowband perfect light absorption in critically coupled metal high-index dielectric cavities\nM ElKabbash, E Ilker, T Letsou, N Hoffman, A Yaney, M Hinczewski, G Strangi\n\n  Abstract\nPerfect light absorption in the visible and near-infrared (NIR) was demonstrated using metamaterials, plasmonic nanostructures, and thin films. Thin film absorbers offer a simple and low-cost design as they can be produced on large areas and without lithography. Light is strongly absorbed in thin film metal-dielectric-metal (MDM) cavities at their resonance frequencies. However, a major drawback of MDM absorbers is their strong resonance iridescence, i.e., angle dependence. Here, we solve the iridescence problem by achieving angle-insensitive narrowband perfect and near-perfect light absorption. In particular, we show analytically that using a high-index dielectric in MDM cavities is sufficient to achieve angle-insensitive cavity resonance. We demonstrate experimentally angle-insensitive perfect and near-perfect absorbers in the NIR and visible regimes up to ±60°. By overcoming the iridescence problem, we open the door for practical applications of MDM absorbers at optical frequencies.\n\n","path":null}]