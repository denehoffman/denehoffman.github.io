<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="text/html; charset=UTF-8" http-equiv="content-type"/>
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
<meta name="robots" content="index, follow">
<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  }
};
</script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>












<title>The BFGS Algorithm Family in Rust (Part 2)</title>



<meta name="title" content="The BFGS Algorithm Family in Rust (Part 2)">


<meta name="author" content="Nathaniel D. Hoffman">


<meta property="og:type" content="website">
<meta property="og:url" content="https://denehoffman.com/blog/the-bfgs-algorithm-family-in-rust-part-2/">

<meta property="og:site_name" content="">


<meta property="og:title" content="The BFGS Algorithm Family in Rust (Part 2)">





<meta property="twitter:card" content="summary_large_image">
<meta property="twitter:url" content="https://denehoffman.com/blog/the-bfgs-algorithm-family-in-rust-part-2/">

<meta property="twitter:title" content="The BFGS Algorithm Family in Rust (Part 2)">




<link rel="canonical" href="https://denehoffman.com/blog/the-bfgs-algorithm-family-in-rust-part-2/">




<link rel="stylesheet" type="text/css" href="https://speyll.github.io/suCSS/reset-min.css"/>
<link rel="stylesheet" type="text/css" href="https://speyll.github.io/suCSS/suCSS-min.css"/>
<link rel="stylesheet" type="text/css" href="https://denehoffman.com/css/style.css"/>

<script src="https://denehoffman.com/js/script.js" defer></script>


</head>
<body>
      <header>
          

  


  <nav id="nav-bar">
    
      <a href="&#x2F;" class="">
        
        &lt;home&gt;
      </a>
    
      <a href="&#x2F;blog" class="">
        
        &lt;blog&gt;
      </a>
    
      <a href="&#x2F;publications" class="">
        
        &lt;publications&gt;
      </a>
    
      <a href="&#x2F;cv" class="">
        
        &lt;cv&gt;
      </a>
    
      <a href="&#x2F;projects" class="">
        
        &lt;projects&gt;
      </a>
    
    <div>
      <input type="checkbox" id="theme-toggle" style="display: none;">
      <label for="theme-toggle" id="theme-toggle-label"><svg id="theme-icon" class="icons"><use href="https://denehoffman.com/icons.svg#lightMode"></use></svg></label>
      <audio id="theme-sound">
        <source src="https://denehoffman.com/click.ogg" type="audio/ogg">
      </audio>
    </div>
  </nav>


      </header>
      <main>
          
<div><a href="..">..</a>/<span class="accent-data">the-bfgs-algorithm-family-in-rust-part-2</span></div>
<time datetime="2024-12-15">Published on: <span class="accent-data">2024-12-15</span></time>

<h1>The BFGS Algorithm Family in Rust (Part 2)</h1>



<blockquote>
<p>Since writing the <a href="https://denehoffman.com/blog/the-bfgs-algorithm-family-in-rust-part-1/">previous post</a>, I have made several simplifications to the library that will change how some structs and traits are used here. The biggest change is that I've abandoned generic floats in favor of a feature gate:</p>
<pre data-lang="rust" style="background-color:#2b303b;color:#c0c5ce;" class="language-rust "><code class="language-rust" data-lang="rust"><span>#[</span><span style="color:#bf616a;">cfg</span><span>(</span><span style="color:#bf616a;">not</span><span>(feature = &quot;</span><span style="color:#a3be8c;">f32</span><span>&quot;))]
</span><span style="color:#b48ead;">pub type </span><span>Float = </span><span style="color:#b48ead;">f64</span><span>;
</span><span> 
</span><span>#[</span><span style="color:#bf616a;">cfg</span><span>(feature = &quot;</span><span style="color:#a3be8c;">f32</span><span>&quot;)]
</span><span style="color:#b48ead;">pub type </span><span>Float = </span><span style="color:#b48ead;">f32</span><span>;
</span></code></pre>
<p>With this code, I can turn the <code>f32</code> version of my crate on and off with a feature flag. While generics can be nice, in this instance they were actually causing the crate to be slower (just according to some of my own internal benchmarks, this might not be the case in general) and more difficult to read. Any instances in the last post where there was a generic type <code>T</code> representing floating-point values have been replaced with this <code>Float</code> type, so <code>Status&lt;T&gt;</code> is now just <code>Status</code>, for example.</p>
</blockquote>
<p>Let's talk about the core BFGS algorithm. The details I'm using here can be found in Nocedal and Wright's book <a href="https://doi.org/10.1007/978-0-387-40065-5">"Numerical Optimization"</a> in Chapter 6: Quasi-Newton Methods. In the last post, I alluded to this and described gradient descent, but we should really cover what Newton's method is first.</p>
<h1 id="newton-s-method">Newton's Method</h1>
<p>The core idea of gradient descent was that we get the gradient at the initial point and walk downhill. We can couple this with a line search to figure out the optimal step size to take downhill, but that's about the best we can do with a single derivative. With two derivatives, however, we can do just a bit better (converge faster). First, let's Taylor expand our function around some point $x$:</p>
<p>$$ f(x + t) \approx f(x) + f'(x)t + \frac{1}{2}f''(x)t^2 $$</p>
<p>We can imagine this as a function of just $t$, keeping $x$ fixed, and try to figure out the value of $t$ that minimizes the function. This can be done by realizing that the derivative of $f(x + t)$ with respect to $t$ should be zero at extrema and the second derivative should be positive at a minimum:</p>
<p>$$ 0 = \frac{\text{d}}{\text{d}t} \left(f(x) + f'(x)t  \frac{1}{2} f''(x)t^2 \right) = f'(x) + f''(x)t $$</p>
<p>or $t = -\frac{f'(x)}{f''(x)}$.</p>
<p>In more than one dimension $n$, we write $f'(x) \to \vec{\nabla}f(\vec{x})$ (a $n$-dimensional vector) and $f''(x) \to \nabla^2 f(\vec{x}) = H$. This second derivative term is an $n \times n$ matrix called the Hessian, and the matrix equivalent of the reciprocal here is the matrix inverse. The Hessian is just a matrix of second derivatives, $H_{ij} = \frac{\partial^2 f(\vec{x})}{\partial \vec{x}_i \partial \vec{x}_j}$, so it is symmetric by definition.</p>
<p>The Hessian is also positive definite, which just means that for all nonzero vectors $\vec{q}$,</p>
<p>$$\vec{q}^{\intercal} H \vec{q} &gt; 0$$</p>
<p>With all of this in mind, recall that the gradient descent update step was</p>
<p>$$ \vec{x}_{k+1} = \vec{x}_k - \alpha_k \vec{\nabla} f(\vec{x}_k) $$</p>
<p>so the equivalent second-derivative (Newton) update step is</p>
<p>$$ \vec{x}_{k+1} = \vec{x}_k - \alpha_k H_k^{-1} \vec{\nabla} f(\vec{x}_k) $$</p>
<p>As far as I know, this is the optimal second-order method for numeric optimization (in theory). In practice, it tends to be horribly inefficient to compute a Hessian matrix and subsequently invert it, and this problem gets worse when you increase the number of free parameters. Particularly, we rarely know the analytic first and second derivatives of our objective function (and it could be argued that knowing those would often make optimization trivial). In most cases, we would have to do these derivatives numerically:</p>
<pre data-lang="rust" style="background-color:#2b303b;color:#c0c5ce;" class="language-rust "><code class="language-rust" data-lang="rust"><span>  </span><span style="color:#b48ead;">fn </span><span style="color:#8fa1b3;">hessian</span><span>(&amp;</span><span style="color:#bf616a;">self</span><span>, </span><span style="color:#bf616a;">x</span><span>: &amp;[Float], </span><span style="color:#bf616a;">user_data</span><span>: &amp;</span><span style="color:#b48ead;">mut</span><span> U) -&gt; Result&lt;DMatrix&lt;Float&gt;, E&gt; {
</span><span>      </span><span style="color:#b48ead;">let</span><span> x = DVector::from_column_slice(x);
</span><span>      </span><span style="color:#b48ead;">let</span><span> h: DVector&lt;Float&gt; = x
</span><span>          .</span><span style="color:#96b5b4;">iter</span><span>()
</span><span>          .</span><span style="color:#96b5b4;">map</span><span>(|&amp;</span><span style="color:#bf616a;">xi</span><span>| Float::cbrt(Float::</span><span style="color:#d08770;">EPSILON</span><span>) * (xi.</span><span style="color:#96b5b4;">abs</span><span>() + </span><span style="color:#d08770;">1.0</span><span>))
</span><span>          .collect::&lt;Vec&lt;_&gt;&gt;()
</span><span>          .</span><span style="color:#96b5b4;">into</span><span>();
</span><span>      </span><span style="color:#b48ead;">let mut</span><span> res = DMatrix::zeros(x.</span><span style="color:#96b5b4;">len</span><span>(), x.</span><span style="color:#96b5b4;">len</span><span>());
</span><span>      </span><span style="color:#b48ead;">let mut</span><span> g_plus = DMatrix::zeros(x.</span><span style="color:#96b5b4;">len</span><span>(), x.</span><span style="color:#96b5b4;">len</span><span>());
</span><span>      </span><span style="color:#b48ead;">let mut</span><span> g_minus = DMatrix::zeros(x.</span><span style="color:#96b5b4;">len</span><span>(), x.</span><span style="color:#96b5b4;">len</span><span>());
</span><span>      </span><span style="color:#b48ead;">for</span><span> i in </span><span style="color:#d08770;">0</span><span>..x.</span><span style="color:#96b5b4;">len</span><span>() {
</span><span>          </span><span style="color:#b48ead;">let mut</span><span> x_plus = x.</span><span style="color:#96b5b4;">clone</span><span>();
</span><span>          </span><span style="color:#b48ead;">let mut</span><span> x_minus = x.</span><span style="color:#96b5b4;">clone</span><span>();
</span><span>          x_plus[i] += h[i];
</span><span>          x_minus[i] -= h[i];
</span><span>          g_plus.</span><span style="color:#96b5b4;">set_column</span><span>(i, &amp;</span><span style="color:#bf616a;">self</span><span>.</span><span style="color:#96b5b4;">gradient</span><span>(x_plus.</span><span style="color:#96b5b4;">as_slice</span><span>(), user_data)?);
</span><span>          g_minus.</span><span style="color:#96b5b4;">set_column</span><span>(i, &amp;</span><span style="color:#bf616a;">self</span><span>.</span><span style="color:#96b5b4;">gradient</span><span>(x_minus.</span><span style="color:#96b5b4;">as_slice</span><span>(), user_data)?);
</span><span>          </span><span style="color:#b48ead;">for</span><span> j in </span><span style="color:#d08770;">0</span><span>..=i {
</span><span>              </span><span style="color:#b48ead;">if</span><span> i == j {
</span><span>                  res[(i, j)] = (g_plus[(i, j)] - g_minus[(i, j)]) / (</span><span style="color:#d08770;">2.0 </span><span>* h[i]);
</span><span>              } </span><span style="color:#b48ead;">else </span><span>{
</span><span>                  res[(i, j)] = ((g_plus[(i, j)] - g_minus[(i, j)]) / (</span><span style="color:#d08770;">4.0 </span><span>* h[j]))
</span><span>                      + ((g_plus[(j, i)] - g_minus[(j, i)]) / (</span><span style="color:#d08770;">4.0 </span><span>* h[i]));
</span><span>                  res[(j, i)] = res[(i, j)];
</span><span>              }
</span><span>          }
</span><span>      }
</span><span>      Ok(res)
</span><span>  }
</span></code></pre>
<p>I'm not sure if I've written this function as efficiently as possible, but already we can see the issue. To accurately calculate the Hessian, we need to take the gradient in two places for each free parameter (<code>g_plus</code> and <code>g_minus</code>), a process which requires at least two function calls for each free parameter. That's a total of $4n^2$ function evaluations for $n$ free parameters. If the function takes one second to evaluate and has ten free parameters, the gradient will take twenty seconds, but the Hessian will take over six minutes. Now it might seem silly to speculate on a function which takes a full second to evaluate (at that point, we can expect any optimization problem is going to take a while), but the real issue is scaling. If we add a single free parameter to this problem, it now takes eight minutes. Most of the problems I deal with in my research can be evaluated in less than 200ms, but they contain over 40 free parameters. Do the math, that's 21 minutes for a single evaluation of the Hessian, and we have to do that at every step in the optimization!</p>
<p>Fortunately, there's a better, faster way. Rather than calculating the full Hessian, let's just approximate it! After all, in the limit of very small steps, the Hessian shouldn't be changing that much (for nice functions), so it makes sense that there should be some way to approximate the next Hessian given the current (and the gradients at both points). This is what puts the "quasi" in quasi-Newton methods. We outline some way of approximating the current Hessian given some history of past Hessian approximates and gradients and apply Newton's method for optimization.</p>
<p>Let's refer to this approximate Hessian as $B_k$<sup class="footnote-reference"><a href="#1">1</a></sup>. Furthermore, following the derivation in Nocedal and Wright, let's write the second-order Taylor expansion of our function at the point $\vec{x}_k$. I'll use $f_k \equiv f(\vec{x}_k)$ to represent the objective function and $\vec{g}_k \equiv \vec{\nabla}f(\vec{x}_k)$ to represent the gradient.</p>
<p>$$
m_k(\vec{p}) = f_k + \vec{g}_k^{\intercal} \vec{p} + \frac{1}{2} \vec{p}^{\intercal} B_k \vec{p}
$$</p>
<p>This approximation is identical to our function when $\vec{p} = 0$, and furthermore, the gradient is also the same. The only difference is in the second derivatives, which are approximately the same. We can assume that we'll use some line search to find the optimal step length $\alpha_k$ to get to the new point, $\vec{x}_{k+1} = \vec{x}_k + \alpha_k \vec{p}_k$. Let's go to this next point and write out the Taylor expansion for the next step here:</p>
<p>$$
m_{k+1}(\vec{p}) = f_{k+1} + \vec{g}_{k+1}^{\intercal} \vec{p} + \frac{1}{2} \vec{p}^{\intercal} B_{k+1} \vec{p}
$$</p>
<p>Now imagine we took this step and looked back at the old position and calculated the gradient using this new expansion evaluated at the old position. We would hope to get the same result as when we used our old expansion! We can actually make this a constraint on our matrix $B$. To look back at the old position, we evaluate the function at $p = -\alpha_k\vec{p}_k$, since this is the step we just took but reversed in direction. We can then write the gradient as</p>
<p>$$
\vec{\nabla} m_{k+1}(-\alpha_k\vec{p}_k) = \vec{g}_{k+1} - \alpha_k B_{k+1} \vec{p}_k
$$</p>
<p>and with the requirement that $ \vec{\nabla} m_{k+1}(-\alpha_k\vec{p}_k) = \vec{g}_k $, we get a new rule for our approximate Hessian:</p>
<p>$$
B_{k+1}\alpha_k\vec{p}_k = \vec{g}_{k+1} - \vec{g}_k
$$</p>
<p>There is some conventional notation to this that we will use throughout the derivation. Rather than using each individual gradient and position, it is helpful to just think about the change in gradient and position, or</p>
<p>$$ \vec{s}_k = \vec{x}_{k+1} - \vec{x}_k = \alpha_k\vec{p}_k $$</p>
<p>and</p>
<p>$$ \vec{y}_k = \vec{g}_{k+1} - \vec{g}_k $$</p>
<p>In this notation, the gradient requirement (called the secant equation), is as follows:</p>
<p>$$ B_{k+1}\vec{s}_k = \vec{y}_k $$</p>
<p>Remember when we said the Hessian is positive definite? We would like our Hessian approximate to be positive definite too. Since we don't include any zero-length jumps, $|\vec{s}_k| &gt; 0$, so we can multiply the previous equation by another $\vec{s}_k$ to find:</p>
<p>$$ \vec{s}_k^{\intercal} B_{k+1} \vec{s}_k &gt; 0 \implies \vec{s}_k^{\intercal} \vec{y}_k &gt; 0 $$</p>
<p>This requirement on $\vec{s}_k$ and $\vec{y}_k$ is called the curvature condition. As it turns out, there's a fairly simple way to ensure this using the Wolfe condition from my previous article. Recall that during our line search, we required that</p>
<p>$$\begin{align}\vec{p}_k^{\intercal} \vec{g}_{k+1} &amp;\geq c_2(\vec{p}_k^{\intercal} \vec{g}_k) \\ \vec{s}_k^{\intercal} \vec{g}_{k+1} &amp;\geq c_2(\vec{s}_k^{\intercal} \vec{g}_k) \\ \vec{s}_k^{\intercal} (\vec{y}_k + \vec{g}_{k}) &amp;\geq c_2(\vec{s}_k^{\intercal} \vec{g}_k) \\ \vec{s}_k^{\intercal} \vec{y}_k + \vec{s}_k^{\intercal} \vec{g}_{k} &amp;\geq c_2(\vec{s}_k^{\intercal} \vec{g}_k) \\ \vec{s}_k^{\intercal} \vec{y}_k &amp;\geq (c_2 - 1)(\vec{s}_k^{\intercal} \vec{g}_k) \\ \vec{s}_k^{\intercal} \vec{y}_k &amp;\geq (c_2 - 1)\alpha_k \vec{p}_k \vec{g}_k \end{align}$$</p>
<p>Since we require $c_2 &gt; 1$ and $\alpha_k &gt; 0$, and assuming $\vec{p}_k$ is somewhat aligned with the gradient (or at a minimum, $\vec{p}_k$ is pointing somewhere downhill), then the curvature condition clearly holds.</p>
<p>Unfortunately, we're only a bit better off than we were when we had no idea what $B$ was. While these conditions limit the space of possible $B$ matrices, the set is still infinite. We need to find more ways to narrow down the space to a unique solution.</p>
<p>One way we could approach this is by assuming that $B$ shouldn't change <em>too much</em> at each update. There are a number of ways of going about this, but the basic idea is that we choose some norm on the change of the matrix between updates with different norms leading to different quasi-Newton algorithms. In particular, minimizing the Frobenius norm for this particular problem will give us the Davidon-Fletcher-Powell (DFP) algorithm. It has a nice update step which we can apply to $B$ that looks like this:</p>
<p>$$B_{k+1} = (I - \rho_k \vec{y}_k \vec{s}_k^{\intercal}) B_k (I - \rho_k \vec{s}_k \vec{y}_k^{\intercal}) + \rho_k \vec{y}_k \vec{y}_k^{\intercal}$$</p>
<p>where $\rho_k = \frac{1}{\vec{y}_k^{\intercal} \vec{s}_k}$. However, we aren't actually done yet, since in the quasi-Newton update step, we really need the matrix $B_{k+1}^{-1}$. We could go and calculate a matrix inverse at each step, but it turns out we don't need to thanks to the Sherman-Morrison-Woodbury formula, which says that the inverse of a rank-$k$ correction on a matrix is equivalent to a rank-$k$ correction on the inverse, or more precisely,</p>
<p>$$(B + UCV)^{-1} = B^{-1} - B^{-1}U(C^{-1}+VB^{-1}U)^{-1}VB^{-1}$$</p>
<p>where $B$ is $n\times n$, $C$ is $k\times k$, $U$ is $n\times k$, and $V$ is $k\times n$. This is a tough one, but we should take the time to work it out. First we expand the right side of the DFP formula:</p>
<p>$$
B_k - \rho_k \vec{y}_k \vec{s}_k^{\intercal} B_k - \rho_k B_k \vec{s}_k \vec{y}_k^{\intercal} + \rho_k^2 \vec{y}_k \vec{s}_k^{\intercal} B_k \vec{s}_k \vec{y}_k^{\intercal} + \rho_k \vec{y}_k \vec{y}_k^{\intercal}
$$</p>
<p>Everything except for the very first $B_k$ should be thought of as those matrices $U$, $V$, and $C$. It is also helpful to know that by a rank-$k$ correction, we mean that the correction is written as $A' = A + \vec{u}_1\vec{v}_1^{\intercal} + \vec{u}_2\vec{v}_2^{\intercal} + ... + \vec{u}_k\vec{v}_k^{\intercal}$. The two terms in the original DFP formula form such a correction of rank-2, since the first terms involve both $\vec{y}_k$ and $\vec{s}_k$ scaled by $\rho_k$ and $B_k$ while the second term only involves $\vec{y}_k$ scaled by $\rho_k$, making it linearly independent. We then require $C$ to be a $2\times 2$ matrix, $U$ to be $n\times 2$, and $V$ to be $2\times n$, where $n$ is the dimensionality of the Hessian. It's a bit tricky to see, but we can write this all in the form:<sup class="footnote-reference"><a href="#2">2</a></sup></p>
<p>$$
B_{k+1} = B_k + \begin{bmatrix}\rho_k\vec{y}_k &amp; \rho_k B_k\vec{s}_k \end{bmatrix} \begin{bmatrix}1 + \rho_k \vec{s}_k^{\intercal} B_k \vec{s}_k &amp; -1 \\ -1 &amp; 0 \end{bmatrix} \begin{bmatrix} \vec{y}_k \\ \vec{s}_k B_k \end{bmatrix}
$$</p>
<p>I'll leave the actual application of the Sherman-Morrison-Woodbury formula as an exercise to the reader, but the conclusion is this rather nice looking update step for the inverse of the (approximate) Hessian:</p>
<p>$$
B_{k+1}^{-1} = B_k^{-1} - \frac{B_k^{-1} \vec{y}_k \vec{y}_k^{\intercal} B_k^{-1}}{\vec{y}_k^{\intercal} B_k^{-1} \vec{y}_k} + \frac{\vec{s}_k \vec{s}_k^{\intercal}}{\vec{y}_k^{\intercal} \vec{s}_k}
$$</p>
<p>At this point, we should be pretty happy. Given some starting value for the inverse Hessian, we have a way of determining a new inverse Hessian at our next step given the value of the old one! We could go and write this up, but then this article would need a different title. It just so happens that you can apply this very same process, but swap out $B$ for $B^{-1}$ at every step (with a new secant condition, $ B_{k+1}^{-1}\vec{y}_k = \vec{s}_k $) and skip that last bit entirely, <em>and it works better in practice</em>! In other words, the update step,</p>
<p>$$ B_{k+1}^{-1} = (I - \rho_k \vec{s}_k \vec{y}_k^{\intercal})B_{k+1}^{-1} (I - \rho_k \vec{y}_k \vec{s}_k^{\intercal}) + \rho_k \vec{s}_k \vec{s}_k^{\intercal} $$</p>
<p>is also a unique solution to our quasi-Newton problem. This is the BFGS update. Of course, we still have the problem of what values should be assigned to $B_0^{-1}$. The neat part about BFGS is that this update step tends to be self-correcting (DFP is generally worse in this regard), assuming the Wolfe condition is met in the line search (this is why I spent so much time on the line search part of the previous post, it really is that important to make educated leaps in the gradient direction!). What this means is that even if we start with a bad initial guess at $B_0^{-1}$, after some number of steps, we should get to a good approximation regardless. We could of course just start with the numerically calculated Hessian, invert it, and go from there, but in practice, this really isn't worth the effort. Instead, we can just use the identity matrix<sup class="footnote-reference"><a href="#3">3</a></sup> for the very first step (just regular gradient descent!) and then correct that identity matrix each time. We will later find that this isn't even the most efficient way of doing things when we look at L-BFGS, but for now we have a formula, let's implement it!</p>
<p>Let's start with a struct to hold our new algorithm:</p>
<pre data-lang="rust" style="background-color:#2b303b;color:#c0c5ce;" class="language-rust "><code class="language-rust" data-lang="rust"><span>#[</span><span style="color:#bf616a;">derive</span><span>(Clone)]
</span><span style="color:#b48ead;">pub struct </span><span>BFGS&lt;U, E&gt; {
</span><span>    </span><span style="color:#bf616a;">x</span><span>: DVector&lt;Float&gt;,
</span><span>    </span><span style="color:#bf616a;">g</span><span>: DVector&lt;Float&gt;,
</span><span>    </span><span style="color:#bf616a;">h_inv</span><span>: DMatrix&lt;Float&gt;,
</span><span>    </span><span style="color:#bf616a;">line_search</span><span>: Box&lt;dyn LineSearch&lt;U, E&gt;&gt;,
</span><span>}
</span><span>
</span><span style="color:#b48ead;">impl</span><span>&lt;U, E&gt; Default </span><span style="color:#b48ead;">for </span><span>BFGS&lt;U, E&gt; {
</span><span>    </span><span style="color:#b48ead;">fn </span><span style="color:#8fa1b3;">default</span><span>() -&gt; </span><span style="color:#b48ead;">Self </span><span>{
</span><span>        </span><span style="color:#b48ead;">Self </span><span>{
</span><span>            x: Default::default(),
</span><span>            g: Default::default(),
</span><span>            h_inv: Default::default(),
</span><span>            line_search: Box::&lt;StrongWolfeLineSearch&gt;::default(),
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>Next, our update formula:</p>
<pre data-lang="rust" style="background-color:#2b303b;color:#c0c5ce;" class="language-rust "><code class="language-rust" data-lang="rust"><span style="color:#b48ead;">impl</span><span>&lt;U, E&gt; BFGS&lt;U, E&gt; {
</span><span>    </span><span style="color:#b48ead;">fn </span><span style="color:#8fa1b3;">update_h_inv</span><span>(&amp;</span><span style="color:#b48ead;">mut </span><span style="color:#bf616a;">self</span><span>, </span><span style="color:#bf616a;">step</span><span>: </span><span style="color:#b48ead;">usize</span><span>, </span><span style="color:#bf616a;">n</span><span>: </span><span style="color:#b48ead;">usize</span><span>, </span><span style="color:#bf616a;">s</span><span>: &amp;DVector&lt;Float&gt;, </span><span style="color:#bf616a;">y</span><span>: &amp;DVector&lt;Float&gt;) {
</span><span>        </span><span style="color:#b48ead;">if</span><span> step == </span><span style="color:#d08770;">0 </span><span>{
</span><span>            </span><span style="color:#bf616a;">self</span><span>.h_inv = </span><span style="color:#bf616a;">self</span><span>.h_inv.</span><span style="color:#96b5b4;">scale</span><span>((y.</span><span style="color:#96b5b4;">dot</span><span>(s)) / (y.</span><span style="color:#96b5b4;">dot</span><span>(y)));
</span><span>        }
</span><span>        </span><span style="color:#b48ead;">let</span><span> rho = Float::recip(y.</span><span style="color:#96b5b4;">dot</span><span>(s));
</span><span>        </span><span style="color:#b48ead;">let</span><span> m_left = DMatrix::identity(n, n) - (y * s.</span><span style="color:#96b5b4;">transpose</span><span>()).</span><span style="color:#96b5b4;">scale</span><span>(rho);
</span><span>        </span><span style="color:#b48ead;">let</span><span> m_right = DMatrix::identity(n, n) - (s * y.</span><span style="color:#96b5b4;">transpose</span><span>()).</span><span style="color:#96b5b4;">scale</span><span>(rho);
</span><span>        </span><span style="color:#b48ead;">let</span><span> m_add = (s * s.</span><span style="color:#96b5b4;">transpose</span><span>()).</span><span style="color:#96b5b4;">scale</span><span>(rho);
</span><span>        </span><span style="color:#bf616a;">self</span><span>.h_inv = (m_left * &amp;</span><span style="color:#bf616a;">self</span><span>.h_inv * m_right) + m_add;
</span><span>    }
</span><span>}
</span></code></pre>
<p>I've modified the <code>Algorithm</code> trait since making the previous post:</p>
<pre data-lang="rust" style="background-color:#2b303b;color:#c0c5ce;" class="language-rust "><code class="language-rust" data-lang="rust"><span style="color:#b48ead;">pub trait </span><span>Algorithm&lt;U, E&gt;: DynClone {
</span><span>    </span><span style="color:#b48ead;">fn </span><span style="color:#8fa1b3;">initialize</span><span>(
</span><span>        &amp;</span><span style="color:#b48ead;">mut </span><span style="color:#bf616a;">self</span><span>,
</span><span>        </span><span style="color:#bf616a;">func</span><span>: &amp;dyn Function&lt;U, E&gt;,
</span><span>        </span><span style="color:#bf616a;">x0</span><span>: &amp;[Float],
</span><span>        </span><span style="color:#bf616a;">bounds</span><span>: Option&lt;&amp;Vec&lt;Bound&gt;&gt;,
</span><span>        </span><span style="color:#bf616a;">user_data</span><span>: &amp;</span><span style="color:#b48ead;">mut</span><span> U,
</span><span>        </span><span style="color:#bf616a;">status</span><span>: &amp;</span><span style="color:#b48ead;">mut</span><span> Status,
</span><span>    ) -&gt; Result&lt;(), E&gt;;
</span><span>    </span><span style="color:#b48ead;">fn </span><span style="color:#8fa1b3;">step</span><span>(
</span><span>        &amp;</span><span style="color:#b48ead;">mut </span><span style="color:#bf616a;">self</span><span>,
</span><span>        </span><span style="color:#bf616a;">i_step</span><span>: </span><span style="color:#b48ead;">usize</span><span>,
</span><span>        </span><span style="color:#bf616a;">func</span><span>: &amp;dyn Function&lt;U, E&gt;,
</span><span>        </span><span style="color:#bf616a;">bounds</span><span>: Option&lt;&amp;Vec&lt;Bound&gt;&gt;,
</span><span>        </span><span style="color:#bf616a;">user_data</span><span>: &amp;</span><span style="color:#b48ead;">mut</span><span> U,
</span><span>        </span><span style="color:#bf616a;">status</span><span>: &amp;</span><span style="color:#b48ead;">mut</span><span> Status,
</span><span>    ) -&gt; Result&lt;(), E&gt;;
</span><span>    </span><span style="color:#b48ead;">fn </span><span style="color:#8fa1b3;">check_for_termination</span><span>(
</span><span>        &amp;</span><span style="color:#b48ead;">mut </span><span style="color:#bf616a;">self</span><span>,
</span><span>        </span><span style="color:#bf616a;">func</span><span>: &amp;dyn Function&lt;U, E&gt;,
</span><span>        </span><span style="color:#bf616a;">bounds</span><span>: Option&lt;&amp;Vec&lt;Bound&gt;&gt;,
</span><span>        </span><span style="color:#bf616a;">user_data</span><span>: &amp;</span><span style="color:#b48ead;">mut</span><span> U,
</span><span>        </span><span style="color:#bf616a;">status</span><span>: &amp;</span><span style="color:#b48ead;">mut</span><span> Status,
</span><span>    ) -&gt; Result&lt;</span><span style="color:#b48ead;">bool</span><span>, E&gt;;
</span><span>    </span><span style="color:#b48ead;">fn </span><span style="color:#8fa1b3;">postprocessing</span><span>(
</span><span>        &amp;</span><span style="color:#b48ead;">mut </span><span style="color:#bf616a;">self</span><span>,
</span><span>        </span><span style="color:#bf616a;">func</span><span>: &amp;dyn Function&lt;U, E&gt;,
</span><span>        </span><span style="color:#bf616a;">bounds</span><span>: Option&lt;&amp;Vec&lt;Bound&gt;&gt;,
</span><span>        </span><span style="color:#bf616a;">user_data</span><span>: &amp;</span><span style="color:#b48ead;">mut</span><span> U,
</span><span>        </span><span style="color:#bf616a;">status</span><span>: &amp;</span><span style="color:#b48ead;">mut</span><span> Status,
</span><span>    ) -&gt; Result&lt;(), E&gt; {
</span><span>        Ok(())
</span><span>    }
</span><span>}
</span></code></pre>
<p>We now just pass the <code>Minimizer</code>'s <code>Status</code> in as a mutable reference and ask the <code>Algorithm</code> to modify it accordingly. The implementation of the BFGS <code>Algorithm</code> is simply:</p>
<pre data-lang="rust" style="background-color:#2b303b;color:#c0c5ce;" class="language-rust "><code class="language-rust" data-lang="rust"><span style="color:#b48ead;">impl</span><span>&lt;U, E&gt; Algorithm&lt;U, E&gt; </span><span style="color:#b48ead;">for </span><span>BFGS&lt;U, E&gt;
</span><span style="color:#b48ead;">where
</span><span>    U: Clone,
</span><span>    E: Clone,
</span><span>{
</span><span>    </span><span style="color:#b48ead;">fn </span><span style="color:#8fa1b3;">initialize</span><span>(
</span><span>        &amp;</span><span style="color:#b48ead;">mut </span><span style="color:#bf616a;">self</span><span>,
</span><span>        </span><span style="color:#bf616a;">func</span><span>: &amp;dyn Function&lt;U, E&gt;,
</span><span>        </span><span style="color:#bf616a;">x0</span><span>: &amp;[Float],
</span><span>        </span><span style="color:#bf616a;">bounds</span><span>: Option&lt;&amp;Vec&lt;Bound&gt;&gt;,
</span><span>        </span><span style="color:#bf616a;">user_data</span><span>: &amp;</span><span style="color:#b48ead;">mut</span><span> U,
</span><span>        </span><span style="color:#bf616a;">status</span><span>: &amp;</span><span style="color:#b48ead;">mut</span><span> Status,
</span><span>    ) -&gt; Result&lt;(), E&gt; {
</span><span>        </span><span style="color:#bf616a;">self</span><span>.f_previous = Float::</span><span style="color:#d08770;">INFINITY</span><span>;
</span><span>        </span><span style="color:#bf616a;">self</span><span>.h_inv = DMatrix::identity(x0.</span><span style="color:#96b5b4;">len</span><span>(), x0.</span><span style="color:#96b5b4;">len</span><span>());
</span><span>        </span><span style="color:#bf616a;">self</span><span>.x = x0;
</span><span>        </span><span style="color:#bf616a;">self</span><span>.g = func.</span><span style="color:#96b5b4;">gradient</span><span>(</span><span style="color:#bf616a;">self</span><span>.x.</span><span style="color:#96b5b4;">as_slice</span><span>(), user_data)?;
</span><span>        status.</span><span style="color:#96b5b4;">inc_n_g_evals</span><span>();
</span><span>        status.</span><span style="color:#96b5b4;">update_position</span><span>((
</span><span>            </span><span style="color:#bf616a;">self</span><span>.x.</span><span style="color:#96b5b4;">as_slice</span><span>(),
</span><span>            func.</span><span style="color:#96b5b4;">evaluate</span><span>(</span><span style="color:#bf616a;">self</span><span>.x.</span><span style="color:#96b5b4;">as_slice</span><span>(), user_data)?,
</span><span>        ));
</span><span>        status.</span><span style="color:#96b5b4;">inc_n_f_evals</span><span>();
</span><span>        Ok(())
</span><span>    }
</span><span>
</span><span>    </span><span style="color:#b48ead;">fn </span><span style="color:#8fa1b3;">step</span><span>(
</span><span>        &amp;</span><span style="color:#b48ead;">mut </span><span style="color:#bf616a;">self</span><span>,
</span><span>        </span><span style="color:#bf616a;">i_step</span><span>: </span><span style="color:#b48ead;">usize</span><span>,
</span><span>        </span><span style="color:#bf616a;">func</span><span>: &amp;dyn Function&lt;U, E&gt;,
</span><span>        </span><span style="color:#bf616a;">bounds</span><span>: Option&lt;&amp;Vec&lt;Bound&gt;&gt;,
</span><span>        </span><span style="color:#bf616a;">user_data</span><span>: &amp;</span><span style="color:#b48ead;">mut</span><span> U,
</span><span>        </span><span style="color:#bf616a;">status</span><span>: &amp;</span><span style="color:#b48ead;">mut</span><span> Status,
</span><span>    ) -&gt; Result&lt;(), E&gt; {
</span><span>        </span><span style="color:#b48ead;">let</span><span> d = -&amp;</span><span style="color:#bf616a;">self</span><span>.h_inv * &amp;</span><span style="color:#bf616a;">self</span><span>.g;
</span><span>        </span><span style="color:#b48ead;">let </span><span>(valid, alpha, f_kp1, g_kp1) = </span><span style="color:#bf616a;">self</span><span>.line_search.</span><span style="color:#96b5b4;">search</span><span>(
</span><span>            &amp;</span><span style="color:#bf616a;">self</span><span>.x,
</span><span>            &amp;d,
</span><span>            Some(</span><span style="color:#bf616a;">self</span><span>.max_step),
</span><span>            func,
</span><span>            bounds,
</span><span>            user_data,
</span><span>            status,
</span><span>        )?;
</span><span>        </span><span style="color:#b48ead;">if</span><span> valid {
</span><span>            </span><span style="color:#b48ead;">let</span><span> dx = d.</span><span style="color:#96b5b4;">scale</span><span>(alpha);
</span><span>            </span><span style="color:#b48ead;">let</span><span> grad_kp1_vec = g_kp1;
</span><span>            </span><span style="color:#b48ead;">let</span><span> dg = &amp;grad_kp1_vec - &amp;</span><span style="color:#bf616a;">self</span><span>.g;
</span><span>            </span><span style="color:#b48ead;">let</span><span> n = </span><span style="color:#bf616a;">self</span><span>.x.</span><span style="color:#96b5b4;">len</span><span>();
</span><span>            </span><span style="color:#bf616a;">self</span><span>.</span><span style="color:#96b5b4;">update_h_inv</span><span>(i_step, n, &amp;dx, &amp;dg);
</span><span>            </span><span style="color:#bf616a;">self</span><span>.x += dx;
</span><span>            </span><span style="color:#bf616a;">self</span><span>.g = grad_kp1_vec;
</span><span>            status.</span><span style="color:#96b5b4;">update_position</span><span>((</span><span style="color:#bf616a;">self</span><span>.x.</span><span style="color:#96b5b4;">as_slice</span><span>(), f_kp1));
</span><span>        } </span><span style="color:#b48ead;">else </span><span>{
</span><span>            status.</span><span style="color:#96b5b4;">set_converged</span><span>();
</span><span>        }
</span><span>        Ok(())
</span><span>    }
</span><span>    </span><span style="color:#65737e;">// other methods aren&#39;t that important for this discussion
</span><span>}
</span></code></pre>
<p>That's pretty much it. Of course, we need to add a few details to tell the algorithm where to stop. Some standard criteria are when the absolute change in the function value or the norm of the gradient goes below some tolerance, and you can see those termination conditions implemented in the full code <a href="https://github.com/denehoffman/ganesh/blob/c5011b52668ba6ee73444bf5a754e51f20142557/src/algorithms/bfgs.rs">here</a>.</p>
<blockquote>
<p>I have to apologize a bit for the amount of changes in the code that have taken place since the last article. I'm developing this in conjunction with a library I'm currently using for amplitude analysis of particle physics data, and being relatively new to Rust, I often find improvements by realizing that I've backed myself into an implementation corner. I hope this article serves as a nice introduction to the BFGS algorithm and how one <em>might</em> implement it in Rust, but it is likely not the best way to implement this or the other algorithms I've discussed. If any readers see places where my code could be improved, I would welcome suggestions <a href="https://github.com/denehoffman/ganesh/issues">via the GitHub repo</a>.</p>
</blockquote>
<p>With that, I'll end this portion of the series. In the next post, I'll discuss the L-BFGS algorithm, the "limited memory" version of BFGS. The surprising thing we will see is that this method tends to work better than BFGS in most practical settings, and its close, bounded cousin, L-BFGS-B also tends to outperform standard BFGS updates. This is mostly because these methods not only reduce the memory required to perform BFGS updates (for large parameter spaces), they tend to involve fewer operations, which makes them more efficient to calculate. The core idea will be, rather than update and store the entire inverse Hessian, just store the changes and update an identity matrix, and while we're at it, just calculate the update step directly from the stored changes to the gradient!</p>
<div class="footnote-definition" id="1"><sup class="footnote-definition-label">1</sup>
<p>This notation is somewhat standard, but if you spend any time reading old papers on this, you'll find that they tend to use $B$, $H$, and their inverses rather interchangeably. For the sake of clarity, $H$ here will always refer to the true Hessian and $B$ to some approximated Hessian.</p>
</div>
<div class="footnote-definition" id="2"><sup class="footnote-definition-label">2</sup>
<p>Credit due to <a href="https://math.stackexchange.com/users/253273/a-%ce%93">A.Γ.</a> in <a href="https://math.stackexchange.com/a/2785578/127253">this post</a> on the Mathematics StackExchange</p>
</div>
<div class="footnote-definition" id="3"><sup class="footnote-definition-label">3</sup>
<p>While this would work, it's recommended to actually use $\frac{\vec{y}_k^{\intercal}\vec{s}_k}{\vec{y}_k^{\intercal}\vec{y}_k}$ for the very first step, after the step has been made but before the first update is performed, and this is reflected in the code.</p>
</div>


<p class="tags-data">
  
</p>

      </main>
      <footer>
          <hr>
<div id="footer-container">
  
  <div>
    <p>Theme and color theme licensed under <a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Licence_MIT">MIT</a>.<br>
      Built with <a target="_blank" rel="noopener noreferrer" href="https://www.getzola.org">Zola</a> using <a target="_blank" rel="noopener noreferrer" href="https://github.com/Speyll/anemone">anemone</a> theme, <a target="_blank" rel="noopener noreferrer" href="https://speyll.github.io/suCSS/">suCSS</a> framework &amp; <a target="_blank" rel="noopener noreferrer" href="https://github.com/Speyll/veqev">veqev</a>.<br>
    </p>

  </div>
  
</div>

      </footer>
</body>
</html>